{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORONAVIRUS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import folium\n",
    "import random\n",
    "import requests\n",
    "import pycountry\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.cm as cm\n",
    "import plotly.express as px\n",
    "from sklearn import metrics\n",
    "from pandas import Timestamp \n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import shuffle\n",
    "import plotly.graph_objects as go\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk import WordNetLemmatizer\n",
    "import plotly.figure_factory as ff\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from plotly.subplots import make_subplots\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONHASHSEED=0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../input/CORD-19-research-challenge/\"\n",
    "CLEAN_DATA_PATH = \"../input/cord-19-eda-parse-json-and-generate-clean-csv/\"\n",
    "\n",
    "pmc_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_pmc.csv\")\n",
    "biorxiv_df = pd.read_csv(CLEAN_DATA_PATH + \"biorxiv_clean.csv\")\n",
    "comm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_comm_use.csv\")\n",
    "noncomm_use_df = pd.read_csv(CLEAN_DATA_PATH + \"clean_noncomm_use.csv\")\n",
    "\n",
    "papers_df = pd.concat([pmc_df,\n",
    "                       biorxiv_df,\n",
    "                       comm_use_df,\n",
    "                       noncomm_use_df], axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORONA_FILE = \"../input/corona-virus-report/covid_19_clean_complete.csv\"\n",
    "\n",
    "full_table = pd.read_csv(CORONA_FILE, parse_dates=['Date'])\n",
    "\n",
    "full_table[['Province/State']] = full_table[['Province/State']].fillna('')\n",
    "full_table['Country/Region'] = full_table['Country/Region'].replace('Mainland China', 'China')\n",
    "full_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']\n",
    "\n",
    "cases = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n",
    "full_table[cases] = full_table[cases].fillna(0)\n",
    "cases = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n",
    "full_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']\n",
    "\n",
    "# replacing Mainland china with just China\n",
    "full_table['Country/Region'] = full_table['Country/Region'].replace('Mainland China', 'China')\n",
    "\n",
    "# filling missing values \n",
    "full_table[['Province/State']] = full_table[['Province/State']].fillna('')\n",
    "full_table[cases] = full_table[cases].fillna(0)\n",
    "\n",
    "# cases in the ships\n",
    "ship = full_table[full_table['Province/State'].str.contains('Grand Princess')|full_table['Country/Region'].str.contains('Cruise Ship')]\n",
    "\n",
    "# china and the row\n",
    "china = full_table[full_table['Country/Region']=='China']\n",
    "row = full_table[full_table['Country/Region']!='China']\n",
    "\n",
    "# latest\n",
    "full_latest = full_table[full_table['Date'] == max(full_table['Date'])].reset_index()\n",
    "china_latest = full_latest[full_latest['Country/Region']=='China']\n",
    "row_latest = full_latest[full_latest['Country/Region']!='China']\n",
    "\n",
    "# latest condensed\n",
    "full_latest_grouped = full_latest.groupby('Country/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\n",
    "china_latest_grouped = china_latest.groupby('Province/State')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\n",
    "row_latest_grouped = row_latest.groupby('Country/Region')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\n",
    "\n",
    "temp = full_table.groupby(['Country/Region', 'Province/State'])['Confirmed', 'Deaths', 'Recovered', 'Active'].max()\n",
    "# temp.style.background_gradient(cmap='Reds')\n",
    "\n",
    "temp = full_table.groupby('Date')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\n",
    "temp = temp[temp['Date']==max(temp['Date'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
