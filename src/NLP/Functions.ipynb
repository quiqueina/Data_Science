{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import re\n",
    "import unidecode\n",
    "import num2words\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, PunktSentenceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from lemmatizer.token_lemmatizer import TokenLemmatizer\n",
    "import json\n",
    "import constants as co"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, word=True, tokenizer=None):\n",
    "    \"\"\"\n",
    "    Tokenize a sentence using the nltk package\n",
    "    :param text: input sentence\n",
    "    :param word:\n",
    "        if word, tokenize at word level (TreebankWordTokenizer by default)\n",
    "        if not, tokenize at sentence level (PunktSentenceTokenizer by default)\n",
    "    :param tokenizer: NLTK Tokenizer object, None by default\n",
    "    :return: detected tokens\n",
    "    Examples:\n",
    "        >>> from nltk.tokenize import TreebankWordTokenizer, PunktSentenceTokenizer\n",
    "        >>> text = 'This is an example sentence'\n",
    "        >>> tokens = tokenize_text(text, word=True)\n",
    "        >>> ['This', 'is', 'an', 'example', 'sentence']\n",
    "        >>>\n",
    "        >>> text = 'First example sentence. Second example sentence'\n",
    "        >>> tokens = tokenize_text(text, word=False)\n",
    "        >>> ['First example sentence.', 'Second example sentence']\n",
    "    \"\"\"\n",
    "    if tokenizer is None and word:\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "    elif tokenizer is None and not word:\n",
    "        tokenizer = PunktSentenceTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_blanks(tokens):\n",
    "    \"\"\"\n",
    "    Remove blanks in a list of tokens\n",
    "    :param tokens: list of tokens\n",
    "    :return: list of tokens without blanks\n",
    "    Example:\n",
    "        >>> tokens = ['This', ' is', '   an   ', 'example ', 'sentence']\n",
    "        >>> remove_blanks(tokens)\n",
    "        >>> ['This', 'is', 'an', 'an', 'example', 'sentence']\n",
    "    \"\"\"\n",
    "    return [token.strip() for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_accents(sentence):\n",
    "    \"\"\"\n",
    "    Remove accents from all the characters (not single symbols) in a sentence\n",
    "    :param sentence: sentence\n",
    "    :return: sentence without accents\n",
    "    Example:\n",
    "        >>> import unidecode\n",
    "        >>> sentence = 'Thís ïs àn éxample ¨¨¨*sentence'\n",
    "        >>> remove_accents(sentence)\n",
    "        >>> 'This is an example ```*sentence'\n",
    "    \"\"\"\n",
    "    return unidecode.unidecode(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(sentence, regex=None):\n",
    "    \"\"\"\n",
    "    Remove punctuation from an input sentence\n",
    "    :param sentence: sentence\n",
    "    :param regex: r'[^a-zA-Z0-9]' by default\n",
    "    :return: sentence without punctuation\n",
    "    Example\n",
    "        >>> import re\n",
    "        >>> sentence = 'This is an example ```*sentence'\n",
    "        >>> remove_punctuation(sentence)\n",
    "        >>> 'Th s  s  n  xample     sentence'\n",
    "    \"\"\"\n",
    "    if regex is None:\n",
    "        regex = r'[^a-zA-Z0-9]'\n",
    "    else:\n",
    "        regex = regex\n",
    "    return re.sub(regex, r' ', sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_unicode_data(sentence):\n",
    "    sentence = unicodedata.normalize('NFKD', sentence).lower().encode('ascii', errors='ignore').decode('utf-8')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stopwords_list=stopwords.words('spanish')):\n",
    "    \"\"\"\n",
    "    Remove the stopwords in a list of tokens\n",
    "    :param tokens: list of tokens\n",
    "    :param stopwords_list: list of tokens. List of spanish stopwords from NLTK by default\n",
    "    :return: list of tokens without stopwords\n",
    "    Example:\n",
    "        >>> from nltk.corpus import stopwords\n",
    "        >>> tokens = ['Esta', 'es', 'una', 'frase', 'de', 'ejemplo']\n",
    "        >>> remove_stopwords(tokens, stop)\n",
    "        >>> ['Esta', 'frase', 'ejemplo']\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_words(tokens, lang='es', to='cardinal', with_decimals=False):\n",
    "    \"\"\"\n",
    "    Converts numbers (like 42) to words (like forty-two) using the num2words package.\n",
    "    It supports multiple languages and also different converters\n",
    "    (see the documentation (https://pypi.org/project/num2words/)\n",
    "    :param tokens: list of tokens\n",
    "    :param lang: see the documentation to see the supported values\n",
    "    :param to: Supported values: 'cardinal', 'ordinal', 'ordinal_num', 'year', 'currency'\n",
    "    :param with_decimals: number with decimals\n",
    "    :return: list of tokens with the numbers converted to words\n",
    "    Example:\n",
    "        >>> import num2words\n",
    "        >>> tokens = ['1', '2', '100', '-1']\n",
    "        >>> num_to_words(tokens)\n",
    "        >>> ['uno', 'dos', 'cien', '-1']\n",
    "        >>>\n",
    "        >>> tokens = ['1992']\n",
    "        >>> num_to_words(tokens, to='year')\n",
    "        >>> ['mil novecientos noventa y dos']\n",
    "    \"\"\"\n",
    "    if with_decimals:\n",
    "        tokens = [num2words.num2words(token, lang=lang, to=to) if token.isdigit() else token for token in tokens]\n",
    "    else:\n",
    "        tokens = [num2words.num2words(int(token), lang=lang, to=to) if token.isdigit() else token for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_stemming(tokens, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stem a list of tokens.\n",
    "    :param tokens: list of tokens\n",
    "    :param stemmer: stemmer to use. PorterStemmer by default\n",
    "    :return: list of computed lexical roots, if not, the tokens\n",
    "    Example:\n",
    "        >>> from nltk.stem import PorterStemmer\n",
    "        >>> tokens = ['pelicula', 'peliculas']\n",
    "        >>> tokens_stemming(tokens)\n",
    "        >>> ['pelicula', 'pelicula']\n",
    "    \"\"\"\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_censured_swearwords(sentence, alias='insult'):\n",
    "    \"\"\"\n",
    "    Standardize all the censored swearwords with an alias\n",
    "    :param sentence: input sentence\n",
    "    :param alias: desired tag to replace the censored word\n",
    "    :return: sentence with censored swearwords standardized with the alias\n",
    "    \"\"\"\n",
    "    sentence = word_tokenize(sentence)\n",
    "    for idx, word in enumerate(sentence):\n",
    "        if '*' in word and word[0].isalpha():\n",
    "            word[idx] = alias\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_token_lemmatization(tokens, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"\n",
    "    Lemmatize a list of tokens. Does not work very well for spanish\n",
    "    :param tokens: list of tokens\n",
    "    :param lemmatizer:lemmatizer to use. WordNetLemmatizer to use\n",
    "    :return: list of detected lexical roots, if not, the tokens\n",
    "    Example:\n",
    "        >>> from nltk.stem import WordNetLemmatizer\n",
    "        >>> tokens = ['pelicula', 'peliculas']\n",
    "        >>> tokens_lemmatization(tokens)\n",
    "        >>> ['pelicula', 'peliculas']\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lemmatization(sentence, language):\n",
    "    \"\"\"\n",
    "    Lemmatize the words in a sentence using the lemmatizer module\n",
    "    :param sentence: input sentence\n",
    "    :param language: desired language. Only available language Spanish for the moment\n",
    "    :return: sentence with the lemmas\n",
    "    \"\"\"\n",
    "    lemmatizer = TokenLemmatizer(language)\n",
    "    sentence = lemmatizer.lemmatize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_entities_from_recognizer_data(json_str):\n",
    "    \"\"\"\n",
    "    >>> get_extraction_entities('{\"score\":0.4292148,\"intent\":\"tef.int.es_ES.mp.tv.search\",\"intents\":[{\"intent\":\"tef.int.es_ES.mp.tv.search\",\"score\":0.4292148}],\"entities\":[{\"type\":\"tef.audiovisual_tvseries_title\",\"entity\":\"el joven Sheldon\",\"label\":\"\",\"canon\":\"el joven sheldon\",\"start_index\":0,\"end_index\":16,\"score\":0.9999999999950252}]}')\n",
    "    :param json_str:\n",
    "    :return: (string_entity, entity_canon, entity_type)\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    try:\n",
    "        json_obj = json.loads(json_str)\n",
    "        values = sorted([(' '.join([ut.convert_num_string(y) for y in x['entity'].split()]), '_'.join(x['canon'].split()), x['type'].split('.')[1]) for x in json_obj['entities']])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_entities_from_ner(phrase, pkg, ner):\n",
    "    \"\"\"\n",
    "    >>> from privatecog_ner import Ner\n",
    "    >>> from privatecog_ner import VERSION\n",
    "    >>> from pkg_resources import parse_version\n",
    "    >>> from privatecog_utils import DEFAULT_CONFIG_NAME\n",
    "    >>> from privatecog_utils import VERSION as UTILS_VERSION\n",
    "    >>> from privatecog_ner import Ner\n",
    "    >>> from privatecog_lib.config.auconfig import privateConfig\n",
    "    >>> from privatecog_utils.misc.notifier import Notifier\n",
    "    >>> from privatecog_utils.repo.model import pricateNlpModelRepository\n",
    "    >>> cfg = privateConfig(DEFAULT_CONFIG_NAME)\n",
    "    >>> ntf = Notifier(level='info')\n",
    "    >>> nlp_repo = privateNlpModelRepository(cfg, 'es-es', 'mh', notifier=ntf)\n",
    "    >>> latest_version = nlp_repo.latest()\n",
    "    >>> pkg = nlp_repo.open(latest_version)\n",
    "    >>> metadata = pkg.release()\n",
    "    >>> ner = Ner('crf', model_data=pkg)\n",
    "    >>> #ner = Ner('gazetteer', model_data=pkg)\n",
    "    >>> detect_entities_dictionary(sentence, pkg, ner)\n",
    "    \"\"\"\n",
    "    # Open an standard (CRF) NER from the data in the NLP package\n",
    "    dict_entities = None\n",
    "    dict_entities = ner(phrase)['entities']\n",
    "    return [(' '.join([num_to_words(y) for y in x['entity'].split()]), '_'.join(x['canon'].split()),\n",
    "             x['type'].split('.')[1]) for x in dict_entities]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_canon(phrase, list_entities):\n",
    "    \"\"\"\n",
    "    >>> transform_input_canon('quiero ver el real madrid', [('real madrid', 'real_madrid', 'ent.audiovisual_sports_team')])\n",
    "    :param phrase:\n",
    "    :param list_entities:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    value = phrase.lower()\n",
    "    for i in range(0, len(list_entities)):\n",
    "        value = value.replace(list_entities[i][0].lower(), \"_\".join(list_entities[i][1].lower().split(\" \")))\n",
    "    return value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_type(phrase, list_entities):\n",
    "    \"\"\"\n",
    "    >>> transform_input_type('quiero ver el real madrid', [('real madrid', 'real_madrid', 'ent.audiovisual_sports_team')])\n",
    "    :param phrase:\n",
    "    :param list_entities:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    value=phrase\n",
    "    for i in range(0, len(list_entities)):\n",
    "        value = value.replace(ut.delete_punctuation(ut.convert_num_string(list_entities[i][1])).lower(), list_entities[i][2])\n",
    "    return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_schedule_time(hour):\n",
    "    \"\"\"\n",
    "    >>> set_schedule_time(14)\n",
    "    :param hour:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if 6 < hour >= 23:\n",
    "        return \"early morning\"\n",
    "    if 6 >= hour < 12:\n",
    "        return \"morning\"\n",
    "    if 12 >= hour < 16:\n",
    "        return \"noon\"\n",
    "    if 16 >= hour < 20:\n",
    "        return \"afternoon\"\n",
    "    if 20 >= hour < 23:\n",
    "        return \"night\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_topic_from_entities(obj, key):\n",
    "    \"\"\"\n",
    "    >>> extract_topic_from_entities(co.dict_entities_topic, 'audiovisual_film_title')\n",
    "    :param obj:\n",
    "    :param key:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    arr = []\n",
    "    results = []\n",
    "\n",
    "    def extract(obj, arr, key):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if isinstance(v, (list)):\n",
    "                    extract(v, arr, key)\n",
    "                if (key in obj[k]):\n",
    "                    arr.append(k)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                extract(item, arr, key)\n",
    "        return arr\n",
    "\n",
    "    for entity in key:\n",
    "\n",
    "        results += extract(obj, arr, entity)\n",
    "    try:\n",
    "        unique_result = list(set(results))[0]\n",
    "    except Exception as e:\n",
    "        unique_result='None'\n",
    "    return unique_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lemmatizer.constants as const\n",
    "\n",
    "\n",
    "class TokenLemmatizer:\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "        self.lemmatizer = self.get_lemmas_dict()\n",
    "\n",
    "    def get_lemmas_dict(self):\n",
    "        lemmas_dict = {}\n",
    "        with open(os.path.join(const.LEMMAS_PATH, const.LANGUAGE_FILES_MAPPING.get(self.language))) as f:\n",
    "            for line in f:\n",
    "                (key, val) = line.split()\n",
    "                lemmas_dict[str(val)] = key\n",
    "        return lemmas_dict\n",
    "\n",
    "    def lemmatize(self, sentence):\n",
    "        sentence = [self.lemmatizer.get(word, word) for word in sentence.split()]\n",
    "        return sentence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
