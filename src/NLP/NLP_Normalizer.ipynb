{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP NORMALIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import emoji\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from ast import literal_eval as lit_eval\n",
    "\n",
    "from config_utils import Config, Log\n",
    "from phrase_modeling.phrase_modeling import PhraseModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 thr_entity=0.8,\n",
    "                 thr_intent=0.8,\n",
    "                 stopwords=stopwords.words('spanish'),\n",
    "                 bg=False):\n",
    "        \"\"\"\n",
    "        :param thr_entity: float, threshold value for filtering low entity classification scores\n",
    "        :param thr_intent: float, threshold value for filtering low intent classification scores\n",
    "        :param entity_filter: dict, Dictionary for mapping entity_type with a frequency of appearances value\n",
    "        :param punctuation: List of strings, punctuation symbols\n",
    "        :param stopwords: List of strings, stopwords provided by NLTK\n",
    "        :param bg: boolean, whether or not to compute phrase modeling over the utterances\n",
    "        \"\"\"\n",
    "        config_path = \"/\".join(os.path.abspath(__file__).split(\"/\")[:-3]) + \"/config/config.ini\"\n",
    "        paths, params, logs = self.get_params(config_path)\n",
    "        self.thr_entity = thr_entity\n",
    "        self.thr_intent = thr_intent\n",
    "        self.stopwords = stopwords\n",
    "        self.detokenizer = TreebankWordDetokenizer()\n",
    "        self.punctuation = str.maketrans({key: \" \" for key in params[\"punctuation\"]})\n",
    "        self.entity_filter = params[\"entity_filter\"]\n",
    "        self.bg = bg\n",
    "        self.not_matched_idx = []\n",
    "\n",
    "        self.log_level = Normalizer.log_level(logs[\"log_level\"])\n",
    "        self.logger, self.log_normalizer, self.path_logs = self.__run_log(logs,\n",
    "                                                                          paths,\n",
    "                                                                          self.log_level)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(config_path):\n",
    "        \"\"\"\n",
    "        This function loads parameters from config.ini file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            config = Config(config_path)\n",
    "\n",
    "            paths = dict()\n",
    "            params = dict()\n",
    "            logs = dict()\n",
    "\n",
    "            paths[\"general_path\"] = \"/\".join(os.path.abspath(__file__).split(\"/\")[:-1])\n",
    "            paths[\"log_path\"] = \"/\".join(os.path.abspath(__file__).split(\"/\")[:-2]) +\\\n",
    "                                \"/logs\"\n",
    "\n",
    "            params[\"punctuation\"] = eval(config.get_config(\"FILTER_PARAMS\", \"PUNCTUATION\"))\n",
    "            params[\"entity_filter\"] = eval(config.get_config(\"FILTER_PARAMS\", \"ENTITY_FILTER\"))\n",
    "\n",
    "            logs[\"log_level\"] = config.get_config(\"LOGS\", \"LOG_LEVEL\").replace('\"', '')\n",
    "\n",
    "            return paths, params, logs\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    def __run_log(self,\n",
    "                  logs,\n",
    "                  paths,\n",
    "                  log_level):\n",
    "        \"\"\"\n",
    "        This function loads log for the process.\n",
    "        Creates a output file where is located the log file and others partial results are downloaded\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            output_file_name = str(datetime.now().strftime('%Y-%m-%d %H:%M:%S')).replace(\" \", \"_\").replace(\":\", \"_\") + \\\n",
    "                               \"_normalizer.log\"\n",
    "            paths[\"log_file_name\"] = output_file_name\n",
    "            logs[\"log_normalizer\"] = paths[\"log_path\"] + \"/\" + paths[\"log_file_name\"]\n",
    "\n",
    "            if not os.path.exists(paths[\"log_path\"]):\n",
    "                os.chmod(paths[\"general_path\"], 0o777)\n",
    "                os.makedirs(paths[\"log_path\"])\n",
    "                os.chmod(paths[\"log_path\"], 0o777)\n",
    "\n",
    "            log = Log(\"Normalizer_EmbeddingsEvaluation\", log_level, logs[\"log_normalizer\"])\n",
    "\n",
    "            logger = log.set_log()\n",
    "            return logger, logs[\"log_normalizer\"], paths[\"log_path\"]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def log_level(log_level):\n",
    "        \"\"\"\n",
    "        Parses log level from config file.\n",
    "        Now posible values are INFO and DEBUG.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if str(log_level).replace('\"', '').lower() == \"debug\":\n",
    "                return logging.DEBUG\n",
    "            else:\n",
    "                return logging.INFO\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    def normalize(self, df,tqdm_call=tqdm):\n",
    "        '''\n",
    "        :param df: pandas DataFrame\n",
    "        :return df_proc: pandas DataFrame\n",
    "        '''\n",
    "        tqdm_call().pandas()\n",
    "\n",
    "        # Filter rows with column RECOGNIZER_ID == 'private-command-recognizer'\n",
    "        self.logger.info(\"Filter rows with column RECOGNIZER_ID == 'private-command-recognizer'\")\n",
    "        df.drop(df.loc[(df.RECOGNIZER_ID == 'private-command-recognizer') | (df['OUTPUT'].astype(str) == 'nan')].index,axis=0,inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # Filter rows by Intent classifier score\n",
    "        # Create UNDER_THR column where 1 means that the thr has not been exceeded (candidates for being removed) and\n",
    "        # 0 otherwise\n",
    "        self.logger.info(\"Filter rows by Intent classifier score\")\n",
    "        df['UNDER_THR'] = np.where((df['SCORE_NU'] < self.thr_intent) | np.isnan(df['SCORE_NU']),1,0)\n",
    "        df.drop(df.loc[(df['SCORE_NU'] < self.thr_intent) | (np.isnan(df['SCORE_NU']))].index,axis=0,inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        # Create entity map column\n",
    "        self.logger.info(\"Creating Entity_map\")\n",
    "        df['ENTITY_MAP'] = df['OUTPUT'].progress_apply(lambda x: self.__get_extraction_entities(x))\n",
    "\n",
    "        # Apply entity frequency filter\n",
    "        self.logger.info(\"Entity frequency filter\")\n",
    "        df['ENTITY_MAP'] = self.__entity_freq_filt(df)\n",
    "\n",
    "        # Lowercase and remove left and right spaces\n",
    "        self.logger.info(\"Lowercasing and removing spaces\")\n",
    "        df['INPUT_PROC'] = df['INPUT'].str.strip().str.lower()\n",
    "\n",
    "        # Remove diacritical marks, remove punctuation, replace emojis by special token\n",
    "        self.logger.info(\"Remove diacritical marks, remove punctuation, replace emojis by special token\")\n",
    "        df['INPUT_PROC'] = \\\n",
    "            df['INPUT_PROC'].progress_apply(lambda x: re.sub(\" +\",\" \",unidecode(str(x)).translate(self.punctuation))\n",
    "            if emoji.emoji_count(str(x)) == 0 else unidecode(re.sub(r'\\:(.+?)\\:','<emoji>',emoji.demojize(str(x).translate(self.punctuation)))))\n",
    "\n",
    "        # Create INPUT_ENTITY column by replacing the entity_string by its entity_type\n",
    "        # Create INPUT_PROC_BG_1 aggregating multiple-word entities in a single token (bigram, trigram, ...)\n",
    "        # based on those recognized by NER model\n",
    "        self.logger.info(\"Create INPUT_ENTITY \")\n",
    "        df[['INPUT_ENTITY', 'INPUT_PROC_BG_1']] = df.progress_apply(lambda x: self.__replace_entities(x),\n",
    "                                                                    axis=1)\n",
    "\n",
    "        # If the entity was not matched in the input string because some exception occurred, update the UNDER_THR value\n",
    "        # to 1\n",
    "        df.loc[df['index'].isin(self.not_matched_idx), 'UNDER_THR'] = 1\n",
    "\n",
    "        # Phrase modeling if requiered\n",
    "        if self.bg:\n",
    "            self.logger.info(\"Applying Phrase Modeling.\")\n",
    "            bgm = PhraseModeling()\n",
    "            df = bgm.fit(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def __replace_entities(self, df):\n",
    "        \"\"\"\n",
    "        Match entity strings with those stored in the entity map for replacing the strings by its entity_type in the\n",
    "         utterance.\n",
    "        :param df: pandas DataFrame\n",
    "        :return ut: string, utterance where the entities that appear in it have been replaced by its tag\n",
    "        \"\"\"\n",
    "        # Processed utterance where we are going to search the entities stored as values in the ent_map\n",
    "        input_proc = df['INPUT_PROC']\n",
    "        # Dictionary of ent_type:[ent_str]\n",
    "        ent_map = df['ENTITY_MAP']\n",
    "        replace_dict = {}\n",
    "        replace_dict_2 = {}\n",
    "        try:\n",
    "            # Append in a list every ent_str that match with the input_proc\n",
    "            # TODO: re.sub(\"_\", \" \", v) --> v\n",
    "            matches = [re.search(re.escape(re.sub(\"_\", \" \", v)), input_proc) for vlist in ent_map.values() for v in vlist]\n",
    "            # matches = [re.search(re.escape(v),input_proc) for vlist in ent_map.values() for v in vlist]\n",
    "            if len(matches) > 0:\n",
    "                # Construct the dictionary escaped(ent_str):ent_type\n",
    "                for match in matches:\n",
    "                    ent_str = input_proc[match.span()[0]:match.span()[1]]\n",
    "                    for key in ent_map.keys():\n",
    "                        # TODO: ent_str\n",
    "                        if re.sub(\" \", \"_\", ent_str) in ent_map[key]:\n",
    "                            ent_tag = key\n",
    "                        else:\n",
    "                            continue\n",
    "                    replace_dict[re.escape(ent_str)] = \"[\" + ent_tag + \"]\"\n",
    "                    # TODO: replace_dict_2[re.escape(ent_str)] = ent_str\n",
    "                    replace_dict_2[re.escape(ent_str)] = re.sub(\" \", \"_\", ent_str)\n",
    "                # Compile the patterns to find (ent_str)\n",
    "                pattern = re.compile(\"|\".join(replace_dict.keys()))\n",
    "                pattern_2 = re.compile(\"|\".join(replace_dict_2.keys()))\n",
    "                # Replace in single step multiple matches\n",
    "                input_entity = pattern.sub(lambda m: replace_dict[re.escape(m.group(0))], input_proc)\n",
    "                input_proc_bg_1 = pattern_2.sub(lambda m: replace_dict_2[re.escape(m.group(0))], input_proc)\n",
    "                return pd.Series([input_entity, input_proc_bg_1])\n",
    "            else:\n",
    "                return pd.Series([input_proc, input_proc])\n",
    "        except Exception as e:\n",
    "            self.logger.exception(e)\n",
    "            # Store in a list the indexes which raised an exception for removing them\n",
    "            self.not_matched_idx.append(df['index'])\n",
    "            # Log the raised exceptions for future corrections\n",
    "            self.logger.debug(str(df.idx) + '\\t' + str(df.AURA_ID) + '\\n' +\n",
    "                              str(df.INPUT) + '\\n' + df.OUTPUT + '\\n' + str(df['ENTITY_MAP']) +\n",
    "                              '\\n' + df.INPUT_PROC + '\\n\\n')\n",
    "\n",
    "\n",
    "    def __get_extraction_entities(self,json_str):\n",
    "        \"\"\"\n",
    "        Generate dictionary with pairs: entity_type (key): list of entity_str (value)\n",
    "        :param json_str: string, dictionary format as a string\n",
    "        :param thr_entity: float, threshold for filtering low score entity values\n",
    "        :return ent_map: dict, entity_tag(key) : list of entity strings (value)\n",
    "        \"\"\"\n",
    "        ent_map = {}\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            for d in json_obj['entities']:\n",
    "                # Filter by entity score. If the thr is not exceeded, the token is not considered as a recognized entity\n",
    "                if d['score'] >= self.thr_entity:\n",
    "                    # Normalize the string of the entity\n",
    "                    value = unidecode(\n",
    "                        self.detokenizer.detokenize(d['entity'].split(\" \")).strip().lower()).translate(\n",
    "                        self.punctuation)\n",
    "                    ent_type = d['type'].split(\".\")\n",
    "                    if ent_type[0] == 'tef':\n",
    "                        ent_type[0] = 'ent'\n",
    "                    ent_type = \".\".join(ent_type)\n",
    "                    if ent_type in ent_map:\n",
    "                        # TODO: Convert numeric strings to words avoiding the problem when it's treated as float\n",
    "                        # TODO: ent_map[ent_type].append(value)\n",
    "                        ent_map[ent_type].append(re.sub(\" \", \"_\", value))\n",
    "                    else:\n",
    "                        # TODO: ent_map[ent_type] = [value]\n",
    "                        ent_map[ent_type] = [re.sub(\" \", \"_\", value)]\n",
    "                else:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            self.logger.exception(e)\n",
    "\n",
    "        return ent_map\n",
    "\n",
    "\n",
    "    def build_entity_freq_dict(self,df):\n",
    "        \"\"\"\n",
    "        Construct entity frequency dictionary\n",
    "        :param df: pandas DataFrame\n",
    "        :return entities_dict: Dictionary\n",
    "        \"\"\"\n",
    "        entities_dict = {}\n",
    "        for d in df.ENTITY_MAP:\n",
    "            if isinstance(d, str):\n",
    "                d = lit_eval(d)\n",
    "            for k, v in d.items():\n",
    "                _dict = entities_dict.get(k, {})\n",
    "                for val in v:\n",
    "                    _num_occurrences = _dict.get(val, 0)\n",
    "                    _dict[val] = _num_occurrences + 1\n",
    "                entities_dict[k] = _dict\n",
    "        return entities_dict\n",
    "\n",
    "    def filter_entity_dict(self,entity_dict):\n",
    "        return {k: {k2: v2 for k2, v2 in v.items() if v2 > self.entity_filter[k]} for k, v in entity_dict.items()}\n",
    "\n",
    "\n",
    "    def __entity_freq_filt(self,df):\n",
    "        \"\"\"\n",
    "        Construct frequency dictionary of entities and filter by a min_count parameter\n",
    "        :param df: pandas DataFrame\n",
    "        :return df: pandas DataFrame, modified input df removing from entity_map column entities which have not reached\n",
    "         min_count threshold\n",
    "        \"\"\"\n",
    "        # Build entity frequency dictionary\n",
    "        entities_dict = self.build_entity_freq_dict(df)\n",
    "\n",
    "        # Filter entity dict\n",
    "        self.entities_dict_filt = self.filter_entity_dict(entities_dict)\n",
    "\n",
    "        # Remove entites which have not reached min_count appearences\n",
    "        new_entity_map = []\n",
    "        for d in df['ENTITY_MAP']:\n",
    "            new_d = {}\n",
    "            for k, v in d.items():\n",
    "                for e in v:  # for every value in the list of entities\n",
    "                    if e in self.entities_dict_filt[k]:\n",
    "                        if k in new_d:\n",
    "                            new_d[k].append(e)\n",
    "                        else:\n",
    "                            new_d[k] = [e]\n",
    "                    else:\n",
    "                        continue\n",
    "            new_entity_map.append(new_d)\n",
    "\n",
    "        return new_entity_map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
