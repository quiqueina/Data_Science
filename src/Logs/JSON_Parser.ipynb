{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(df):\n",
    "\t# Convert string to JSON dict\n",
    "\toutput_df = df.OUTPUT.apply(lambda x: json.loads(x or '{}'))\n",
    "\t# JSON to DataFrame: Create a dataframe with each key as a new column\n",
    "\toutput_df = pd.DataFrame(output_df.to_list())\n",
    "\toutput_df = output_df[['score', 'intent', 'intents', 'entities']]  # re-order\n",
    "\toutput_df.rename(columns={'score': 'OUTPUT_score_intent',\n",
    "\t\t\t\t\t\t\t  'intent': 'OUTPUT_intent',\n",
    "\t\t\t\t\t\t\t  'intents': 'OUTPUT_intents',\n",
    "\t\t\t\t\t\t\t  'entities': 'OUTPUT_entities'}, inplace=True)  # re-name 'score' columns\n",
    "\n",
    "\treturn output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_entity(output_df):\n",
    "\t# Unstack 'entities' list of entities: for each utterance, create a new entry per entity detected\n",
    "\tentities_df = output_df['OUTPUT_entities'].apply(lambda x: pd.Series(x)).stack().reset_index(level=1,drop=True).to_frame('ENTITIES_SINGLE_RAW')\n",
    "\t# JSON to DataFrame: Create a dataframe with each key as a new column\n",
    "\tentities_df = pd.DataFrame(entities_df['ENTITIES_SINGLE_RAW'].to_list(),index=entities_df.index)  # explicitly pass the index because when doing .to_list we lose the indices, and we need to keep the reference of exploded/stacked entitites\n",
    "\tentities_df = entities_df[['type', 'entity', 'label', 'canon', 'start_index', 'end_index', 'score']]  # re-order\n",
    "\tentities_df.rename(columns={'score': 'ENTITY_score',\n",
    "\t\t\t\t\t\t\t\t'type': 'ENTITY_type',\n",
    "\t\t\t\t\t\t\t\t'entity': 'ENTITY_entity',\n",
    "\t\t\t\t\t\t\t\t'label': 'ENTITY_label',\n",
    "\t\t\t\t\t\t\t\t'canon': 'ENTITY_canon',\n",
    "\t\t\t\t\t\t\t\t'start_index': 'ENTITY_start_index',\n",
    "\t\t\t\t\t\t\t\t'end_index': 'ENTITY_end_index'}, inplace=True)  # re-name 'score' columns\n",
    "\n",
    "\treturn entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete entries with repeated CORR_ID (due to bugs). In the future, this function won't be necessary\n",
    "\n",
    "def delete_repeated_entries(df):\n",
    "\t# Fix (or bypass) for a bug that assigns the same CORR_ID to different samples (CORR_ID should be a unique identifier)\n",
    "\tcorr_id_count = df.groupby('CORR_ID').CORR_ID.count()  # count the appearance of each CORR_ID\n",
    "\tCORR_ID_BUGs = corr_id_count[\n",
    "\t\tcorr_id_count == 2].index.to_list()  # get a list with all CORR_IDs repeated exactly twice\n",
    "\tdf = df.drop(df.loc[df.CORR_ID.isin(CORR_ID_BUGs)].index.to_list())  # delete all entries with these CORR_IDs\n",
    "\n",
    "\t# Fix (or bypass) for a bug that repeats n-times an entry with the same CORR_ID, but only the last is valid\n",
    "\tCORR_ID_REP = df.loc[df.duplicated('CORR_ID','last')].index.to_list()  # get a list with the indices of all repeated entries (but the 'last' of each repetition)\n",
    "\tdf = df.drop(CORR_ID_REP)  # delete all entries with these CORR_IDs\n",
    "\n",
    "\t# Reset indices to avoid possible problems due to conversions from \"list to dataframe\", where indices are lost, and posterior concatenations of dataframes\n",
    "\tdf = df.reset_index(drop=True)\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarizes columns for all dataframes according to latest version\n",
    "\n",
    "def standarize_columns(df, filename):\n",
    "\tdf_date = filename.split(\"/\")[-1].split(\"-\")[3].split(\".\")[0]  # get dataframe's year-month\n",
    "\n",
    "\tdf = df.rename(columns={'num_file': 'numFile'})  # rename num_file column to numFile\n",
    "\tdf = df.reindex(columns=headers_canon)  # add missing columns to avoid columns mismatch when concatenating\n",
    "\tif df_date != '201905':  # for all log files but May (the most complete one according to header's canon)...\n",
    "\t\tdf.userType = 'S'  # ...artificially fill all users as 'S' users (standard users)\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "def format_logs(data_path):\n",
    "    ## ENTITIES\n",
    "    file_name_ent = 'PARSED-ES-RECOGNIZER-NLP-concat.csv.bz2'\n",
    "    file_path_ent = os.path.join(data_path, file_name_ent)\n",
    "    # DataFrame loading\n",
    "    dfEnt = pd.read_csv(file_path_ent, encoding='utf-8', parse_dates=['RECOGNIZER_DT'], dtype={'REASON': str, 'USER_ID_GLOBAL': str, 'INTENT_RAW': str}, sep=',') # necessary to specify dtype because, when parsing this columns, first values from which pandas infer types are empty, and so does not detect dtype properly\n",
    "    dfEnt['DOMAIN'] = dfEnt.INTENT.str.split('.').str[0] # create a new column indicating the domain of the query/utterance\n",
    "    dfEnt['RECOGNIZER_DT'] = dfEnt['RECOGNIZER_DT'].dt.tz_convert(tz='Europe/Madrid') # convert UTC time to local time\n",
    "\n",
    "    ## INTENTS\n",
    "    file_name_int = 'ES-RECOGNIZER-NLP-concat.csv.bz2'\n",
    "    file_path_int = os.path.join(data_path, file_name_int)\n",
    "    # DataFrame loading\n",
    "    dfInt = pd.read_csv(file_path_int, encoding='utf-8', parse_dates=['RECOGNIZER_DT'], dtype={'REASON': str, 'USER_ID_GLOBAL': str, 'INTENT_RAW': str}, sep=',')\n",
    "    dfInt['DOMAIN'] = dfInt.INTENT.str.split('.').str[0] # create a new column indicating the domain of the query/utterance\n",
    "    dfInt['RECOGNIZER_DT'] = dfInt['RECOGNIZER_DT'].dt.tz_convert(tz='Europe/Madrid') # convert UTC time to local time\n",
    "\n",
    "    ## ENTITIES\n",
    "    dfEnt = dfEnt.loc[~(dfEnt.INTENT.isnull() & dfEnt.ENTITIES.isnull()) & ~dfEnt.USER_ID_GLOBAL.isna()] # keep only those entries with intent, entity, or intent and entity (all except \"empty intent AND empty entity\"), and AURA_ID_GLOBAL not NaN\n",
    "    dfEnt['ENTITY_type'] = dfEnt.ENTITY_type.fillna('intent_but_no_entity') # necessary for posteriory labelEncoder not tu crash (if it receives an empty string, throws an error) [empty entities happen in cases such as tv.on, common.greetings, etc...]. Also, in order to be able to cross fields with dfInt, fill ENTITY_type nulls instead of removing them.\n",
    "\n",
    "    # Transform non-numerical labels to numerical labels\n",
    "    le_ent = preprocessing.LabelEncoder() # create label encoder\n",
    "    numeric_ent = le_ent.fit_transform(dfEnt.ENTITY_type) # fit label encoder with the categorical values/non-numerical labels, and transform all labels to numerical labels\n",
    "    le_dom_ent = preprocessing.LabelEncoder() # create label encoder\n",
    "    numeric_dom = le_dom_ent.fit_transform(dfEnt.DOMAIN) # fit label encoder with the categorical values/non-numerical labels, and transform all labels to numerical labels\n",
    "\n",
    "    # Gather relevant timestamp info\n",
    "    recognizer_dt = dfEnt.RECOGNIZER_DT # get timestamp for each entry\n",
    "    time_info = [recognizer_dt.dt.year.rename('year'), recognizer_dt.dt.month.rename('month'), recognizer_dt.dt.weekday.rename('weekday'), recognizer_dt.dt.hour.rename('hour')] # extract year, month, weekday and hour of the day for each entry\n",
    "    time_info = pd.concat(time_info, axis = 1) # create a dataframe with this information\n",
    "    recognizer_dt = recognizer_dt.to_frame().join(time_info) # create a unique recognizer_dt dataframe with all relevant information relative to timestamp\n",
    "\n",
    "    # Build entity-datetime dataframe\n",
    "    ent_date_df = pd.DataFrame({'entity_num': numeric_ent, 'domain_num': numeric_dom}, index = dfEnt.index).\\\n",
    "        join([recognizer_dt, dfEnt.AURA_ID_GLOBAL, dfEnt.CORR_ID, dfEnt.ENTITY_type, dfEnt.DOMAIN, dfEnt.CHANNEL_CD, dfEnt.RECOGNIZER_ID, dfEnt.userType, dfEnt.RECOGNIZER_DT.dt.strftime('%Y-%m').rename('year_month'), dfEnt.RECOGNIZER_DT.dt.strftime('%Y-%W').rename('year_week')]).\\\n",
    "        rename(columns = {'RECOGNIZER_DT': 'datetime', 'USER_ID_GLOBAL': 'user_id_global', 'CORR_ID': 'corr_id', 'ENTITY_type': 'entity_label', 'DOMAIN': 'domain_label', 'CHANNEL_CD': 'channel', 'RECOGNIZER_ID': 'recognizer_id', 'userType': 'user_type'}) # create a dataframe with entities-datetime-aura ID global info // # strftime('%b %Y'): All days in a new year preceding the first Monday are considered to be in week 0.\n",
    "    ent_date_df = ent_date_df[['recognizer_id', 'channel', 'corr_id', 'datetime', 'year_week', 'year_month', 'year', 'month', 'weekday', 'hour', 'user_id_global', 'domain_label', 'domain_num', 'entity_label', 'entity_num', 'user_type']] # re-order\n",
    "\n",
    "\n",
    "    ## INTENTS\n",
    "    dfInt = dfInt.loc[~(dfInt.INTENT.isnull() & dfInt.ENTITIES.isnull()) & ~dfInt.AURA_ID_GLOBAL.isna()] # keep only those entries with intent, entity, or intent and entity (all except \"empty intent AND empty entity\"), and AURA_ID_GLOBAL not NaN\n",
    "\n",
    "    # Transform non-numerical labels to numerical labels\n",
    "    le_int = preprocessing.LabelEncoder() # create label encoder\n",
    "    numeric_int = le_int.fit_transform(dfInt.INTENT) # fit label encoder with the categorical values/non-numerical labels, and transform all labels to numerical labels\n",
    "    le_dom_int = preprocessing.LabelEncoder() # create label encoder\n",
    "    numeric_dom = le_dom_int.fit_transform(dfInt.DOMAIN) # fit label encoder with the categorical values/non-numerical labels, and transform all labels to numerical labels\n",
    "\n",
    "    # Gather relevant timestamp info\n",
    "    recognizer_dt = dfInt.RECOGNIZER_DT # get timestamp for each entry\n",
    "    time_info = [recognizer_dt.dt.year.rename('year'), recognizer_dt.dt.month.rename('month'), recognizer_dt.dt.weekday.rename('weekday'), recognizer_dt.dt.hour.rename('hour')] # extract year, month, weekday and hour of the day for each entry\n",
    "    time_info = pd.concat(time_info, axis = 1) # create a dataframe with these information\n",
    "    recognizer_dt = recognizer_dt.to_frame().join(time_info) # create a unique recognizer_dt dataframe with all relevant information relative to timestamp\n",
    "\n",
    "    # Build intent-datetime dataframe\n",
    "    int_date_df = pd.DataFrame({'intent_num': numeric_int, 'domain_num': numeric_dom}, index = dfInt.index).\\\n",
    "        join([recognizer_dt, dfInt.AURA_ID_GLOBAL, dfInt.CORR_ID, dfInt.INTENT, dfInt.DOMAIN, dfInt.CHANNEL_CD, dfInt.RECOGNIZER_ID, dfInt.userType, dfInt.RECOGNIZER_DT.dt.strftime('%Y-%m').rename('year_month'), dfInt.RECOGNIZER_DT.dt.strftime('%Y-%W').rename('year_week')]).\\\n",
    "        rename(columns = {'RECOGNIZER_DT': 'datetime', 'USER_ID_GLOBAL': 'user_id_global', 'CORR_ID': 'corr_id', 'INTENT': 'intent_label', 'DOMAIN': 'domain_label', 'CHANNEL_CD': 'channel', 'RECOGNIZER_ID': 'recognizer_id', 'userType': 'user_type'}) # create a dataframe with entities-datetime-aura ID global info // # strftime('%b %Y'): All days in a new year preceding the first Monday are considered to be in week 0.\n",
    "    int_date_df = int_date_df[['recognizer_id', 'channel', 'corr_id', 'datetime', 'year_week', 'year_month', 'year', 'month', 'weekday', 'hour', 'user_id_global', 'domain_label', 'domain_num', 'intent_label', 'intent_num', 'user_type']] # re-order\n",
    "\n",
    "    return ent_date_df, int_date_df\n",
    "\n",
    "\n",
    "# Path to logs' dataset\n",
    "input_path = '/home/kike/Documentos/data/logs_recognizer/concat'\n",
    "ent_df, int_df = format_logs(input_path)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
