{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSIONALITY REDUCTION USING PRINCIPAL COMPONENTS\n",
    "\n",
    "#### Given a set of features, you want to reduce the number of featureswgile retaining the variance in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction: Reduce the dimensionality of our feature matrix by creating new features with \"ideally\" similar ability to train quality models but with significant fewer dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libreries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "df= digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape[0] of the feature is:  1797 & the Shape[1] is :  64\n"
     ]
    }
   ],
   "source": [
    "# Standarize the feature matrix\n",
    "\n",
    "features  = StandardScaler().fit_transform(df)\n",
    "print(\"The Shape[0] of the feature is: \", str(features.shape[0]) + \" & the Shape[1] is : \", str(features.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA that will retain 99% of the variance\n",
    "\n",
    "pca = PCA(n_components = 0.99, whiten = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape[0] of the feature_pca is:  1797 & the Shape[1] is :  54\n"
     ]
    }
   ],
   "source": [
    "# Conduct PCA\n",
    "\n",
    "features_pca = pca.fit_transform(features)\n",
    "print(\"The Shape[0] of the feature_pca is: \", str(features_pca.shape[0]) + \" & the Shape[1] is : \", str(features_pca.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features :  64\n",
      "Reduced number of features :  54\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "print(\"Original number of features : \", features.shape[1])\n",
    "print(\"Reduced number of features : \", features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Features when data is Linearly NOT separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape[0] of the features_not is:  1000 & the Shape[1] is :  2\n"
     ]
    }
   ],
   "source": [
    "# Create linearly inseparable data\n",
    "feature_not, _ = make_circles(n_samples = 1000, random_state = 1, noise = 0.1, factor = 0.1)\n",
    "print(\"The Shape[0] of the features_not is: \", str(feature_not.shape[0]) + \" & the Shape[1] is : \", str(feature_not.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Shape[0] of the features_not_kpca is:  1000 & the Shape[1] is :  1\n"
     ]
    }
   ],
   "source": [
    "# Apply Kernel PCA with radius basis function(RBF) kernel\n",
    "kpca = KernelPCA(kernel = \"rbf\", gamma = 15, n_components = 1)\n",
    "features_not_kpca = kpca.fit_transform(feature_not)\n",
    "print(\"The Shape[0] of the features_not_kpca is: \", str(features_not_kpca.shape[0]) + \" & the Shape[1] is : \", str(features_not_kpca.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features :  2\n",
      "Reduced number of features :  1\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "print(\"Original number of features : \", feature_not.shape[1])\n",
    "print(\"Reduced number of features : \", features_not_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducind Features by Maximizing Class Separability\n",
    "#### Try Linear Discriminant Analysis (LDA) to project the features onto components axes that maximize the separation of classes \n",
    "Maximizing the difference between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Data Set\n",
    "iris = datasets.load_iris()\n",
    "features_class = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create amd run am LDA, then use it to transform the features \n",
    "lda = LinearDiscriminantAnalysis(n_components = 1)\n",
    "features_lda = lda.fit(features_class, target).transform(features_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features :  4\n",
      "Reduced number of features :  1\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "print(\"Original number of features : \", features_class.shape[1])\n",
    "print(\"Reduced number of features : \", features_lda.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9912126])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the amount of variance explained\n",
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to select N_Components??\n",
    "\n",
    "Set LDA to \"n_componets= None\" to return the ratio of variance explained by every coponent feature, then calculate how many components are required to get above some threshold of variance explained (often 95% or 99%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run LDA\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components= None)\n",
    "features_lda_none = lda.fit(features_class, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array of explained variance ratios\n",
    "\n",
    "lda_var_ratios = lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "\n",
    "def select_n_components(var_ratio, goal_var: float) ->int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    # For the explained variance of each feature\n",
    "    for explained_variance in var_ratio:\n",
    "        # Add the explained variable to the total\n",
    "        total_variance += explained_variance\n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "    # Return the number of components\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Function\n",
    "run = select_n_components(lda_var_ratios, 0.95)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Features Using Matrix Factorization\n",
    "#### Use non negative matrix factorization (NMF) to reduce the dimensionality of the feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "digits = datasets.load_digits()\n",
    "features_digits = digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fit and apply NMF\n",
    "nmf = NMF(n_components = 10, random_state = 1)\n",
    "features_nmf  = nmf.fit_transform(features_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features :  64\n",
      "Reduced number of features :  10\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "print(\"Original number of features : \", features_digits.shape[1])\n",
    "print(\"Reduced number of features : \", features_nmf.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Features on Sparse Data\n",
    "\n",
    "#### You have an sparse Matriz an want to reduce the dimensionality using Truncated Singular Value Decomposition (TSVD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries \n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "df_digits = datasets.load_digits()\n",
    "df = df_digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize feature matrix\n",
    "\n",
    "features_df = StandardScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Sparse Matrix\n",
    "features_sparse = csr_matrix(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TSVD \n",
    "tsvd = TruncatedSVD(n_components = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct TSVD on sparse_matrix\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features :  64\n",
      "Reduced number of features :  10\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "print(\"Original number of features : \", features_sparse.shape[1])\n",
    "print(\"Reduced number of features : \", features_sparse_tsvd.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30039385377447186"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of first three components' explained variance ratio\n",
    "tsvd.explained_variance_ratio_[0:3].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can automate the process by creating a function that runs TSVD with n_components set to one less than the number of origianl features and then calculate the number of components that explain a desired amount of the original dataÅ› variance: \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run an SVD with one less than number of features\n",
    "tsvd = TruncatedSVD(n_components= features_sparse.shape[1]-1)\n",
    "features_tsvd = tsvd.fit(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of explained variances\n",
    "tsvd_var_ratios = tsvd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "\n",
    "def select_n_components(var_ratio, goal_var: float) ->int:\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    # For the explained variance of each feature\n",
    "    for explained_variance in var_ratio:\n",
    "        # Add the explained variable to the total\n",
    "        total_variance += explained_variance\n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        # If we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            # End the loop\n",
    "            break\n",
    "    # Return the number of components\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run function 95%\n",
    "select_n_components(tsvd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run function 99%\n",
    "select_n_components(tsvd_var_ratios, 0.99)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
