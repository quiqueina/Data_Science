{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL EVALUATION\n",
    "\n",
    "# EVALUATING BINARY CLASSIFIER PREDICTIONS\n",
    "\n",
    "Given a trained classification model you want to evaluate its quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring= \"accuracy\".\n",
    "\n",
    "Proportion of observations predicted correctly. In the real world data could suffer from imbalanced classes ( 90% women, 10% men) so that accuracy could suffers from a paradox where a model is highly accurate but lacks predictive power.\n",
    "\n",
    "#### Performance metrics\n",
    "Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "TP: True Positives. Predicted as True being True\n",
    "\n",
    "TN: True Negatives.  Predicted as False being False\n",
    "\n",
    "FP: False Positives. Predicted as True being False        ==> TYPE 1 ERROR \n",
    "\n",
    "FN: False Negatives. Predicted as False being True        ==> TYPE 2 ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Feature Matrix and Target Vector \n",
    "\n",
    "X,y = make_classification(n_samples = 10000,\n",
    "                          n_features = 3,\n",
    "                          n_informative = 3,\n",
    "                          n_redundant = 0,\n",
    "                          n_classes = 2,\n",
    "                          random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression\n",
    "\n",
    "logit = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9555, 0.95  , 0.9585, 0.9555, 0.956 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validate Model Using Accuracy\n",
    "\n",
    "cross_val_score(logit, X , y, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRECISION\n",
    "\n",
    "Precision = TP / (TP*FP)\n",
    "\n",
    "Proportion of every observation predicted to be positive that is actually positive. It's similar to a measure of noise in our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95963673, 0.94820717, 0.9635996 , 0.96149949, 0.96060606])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validate using precision\n",
    "\n",
    "cross_val_score(logit, X, y, scoring = \"precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RECALL\n",
    "\n",
    "Recall = TP / (TP + FN )\n",
    "\n",
    "Proportion of every positive observation that is truly positive. Recall measures the model's ability to identify an observation of the positive class.\n",
    "\n",
    "High recall models are optimistic in that they have a low bar for predicting thtat an obervation is in the positive class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.951, 0.952, 0.953, 0.949, 0.951])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validate using precision\n",
    "\n",
    "cross_val_score(logit, X, y, scoring = \"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 - SCORE\n",
    "\n",
    "F1 = 2 * [(Precision * Recall) / (Precision + Recall)]\n",
    "\n",
    "\n",
    "Balance between Precision and Recall.\n",
    "\n",
    "It is a measure of correctness achieved in positive prediction. Observations labeled as positive that are actually positives\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95529884, 0.9500998 , 0.95827049, 0.95520886, 0.95577889])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validate using F1-Score\n",
    "\n",
    "cross_val_score(logit, X, y, scoring = \"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do we have the true \"y\" values ?\n",
    "We can check accuracy directly with our \"ŷ\" predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values for training target vector                  #ŷ\n",
    "\n",
    "y_hat = logit.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.947"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "\n",
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
