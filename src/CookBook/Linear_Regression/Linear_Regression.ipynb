{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION\n",
    "\n",
    "One of the most simplest supervised learning algorithms in our toolkit.\n",
    "\n",
    "It is a common and useful method of making predictions when the target vector is a quantitative value (home price, age ...) .\n",
    "\n",
    "It assumes that the relationship between the features and the target vector is approximately linear.\n",
    "\n",
    "**The effect** (also called coefficient, weight or parameter) of the features on the target vector is constant.\n",
    "\n",
    "Great interpretability due to coefficients of the models are the effect on a one-unit change on the target vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FITTING A LINE\n",
    "\n",
    "You want to train a model that represents a linear relationship between the feature and the target vector.\n",
    "\n",
    "For the sake of explanation we have trained our model using only two features, this mean our model will be: \n",
    "\n",
    "**ŷ = Bo + B1 * x1 + B2 * x2 + €rror**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD BOSTON == The target value is the median value of a Boston home in the 1970's in thousands of dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with only two features\n",
    "\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]   # All rows, two first columns\n",
    "target = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression\n",
    "\n",
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Linear Regression\n",
    "\n",
    "model = regression.fit(features, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bo == Bias or Intercept\n",
    "B1 and B2 == Coefficients identified by fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.485628113468223"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the intercept Bo\n",
    "\n",
    "Bo = model.intercept_\n",
    "Bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.35207832,  0.11610909])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the feature coefficients \n",
    "\n",
    "coefficients = model.coef_\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first feature in our solution is the number of crimes per resident.\n",
    "\n",
    "The model coefficient of this feature was 0.35, meaning that if we multiply this coefficient by 1000 we have the change in the house price for each additional one crime per capita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For every single crime per capita will decrease the price of the house by approximately :  -352.0783156402677 €\n"
     ]
    }
   ],
   "source": [
    "crime_unit_change = model.coef_[0]*1000\n",
    "print(\"For every single crime per capita will decrease the price of the house by approximately : \",crime_unit_change, \"€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The price of the first home in the dataset is the first value in the target vector * 1000\n",
    "\n",
    "real_price = target[0]*1000\n",
    "real_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24573.366631705547"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using predict we can calculate the value of the house\n",
    "\n",
    "predicted_price = model.predict(features)[0]*1000\n",
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model was off by :  -573.3666317055468 €\n"
     ]
    }
   ],
   "source": [
    "Difference = real_price - predicted_price\n",
    "\n",
    "print(\"The model was off by : \", Difference, \"€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Interactive Effects\n",
    "\n",
    "You have a feature whose effect on the target variable depends on another feature.\n",
    "\n",
    "Solution : Create an interaction term to capture that dependence using scikit-learn's Polynomial Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interection term\n",
    "\n",
    "interaction = PolynomialFeatures(degree = 3, include_bias = False, interaction_only = True)\n",
    "features_interaction = interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Regression \n",
    "\n",
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Linear Regression\n",
    "\n",
    "model = regression.fit(features_interaction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a feature's effect on our target variable is at least partially dependent on another feature.\n",
    "\n",
    "Example: There are two factors that can determine the sweetness of coffe.\n",
    "\n",
    "1-. Add sugar \n",
    "\n",
    "2 -. Stir the glass\n",
    "\n",
    "Both factors are mandatory to find the coffe sweet, if you act with them separately there will be no success.\n",
    "\n",
    "We can account for interaction effects by including a new feature comprising the product of corresponding values from the interacting features.\n",
    "\n",
    "ŷ = Bo + B1*x1 + B2*x2 + B3*x1*x2 + €rror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.32e-03, 1.80e+01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the feature values for the first observation\n",
    "\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an interaction term, we simply multiply this two values together for every observation.\n",
    "\n",
    "**Polynomial Features**\n",
    "\n",
    "Create interactions terms for all combinations of features. We can use model selection strategies to identify the combination of features and interaction terms that produce the best model\n",
    "\n",
    "3 parameters we must see: \n",
    "\n",
    "interaction_only : True ==> Tells Polynomial features to only return interacion terms (no polynomial features)\n",
    "\n",
    "include_bias : False ==> Prevent from containing bias\n",
    "\n",
    "degree: maximun number of features to create interaction terms from. (in case we want interaction between 3 elements) x, x²,x³\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each observation multiply the value of the first and second feature\n",
    "\n",
    "interaction_term = np.multiply(features[:,0], features[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11376"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View interaction term for first observation.\n",
    "\n",
    "interaction_term[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the output of Polynomial features from our solution by checking to see if the first observation's feature values and interaction term value match oir manually calculated verion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.3200e-03, 1.8000e+01, 1.1376e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the values of the first observation\n",
    "\n",
    "features_interaction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a NonLinear Relationship\n",
    "\n",
    "Create a Polynomial Regression by including polynomial features in a linear regression model.\n",
    "\n",
    "Convert this :        ŷ = Bo + B1*x1  \n",
    "\n",
    "into this:   ŷ = Bo + B1*x1 + B2*x1^2 + ... +  Ba*Xj^d + €rror             being d the degree of the polynomial.\n",
    "\n",
    "The model will be more flexible by adding a esisting feature to some power x², x³, linear regression will interpret this values as any other one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_non_linear = boston.data[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Createa a polynomial feature x^2 and x^3\n",
    "\n",
    "polynomial = PolynomialFeatures(degree = 3, include_bias = False)\n",
    "features_polynomial = polynomial.fit_transform(features_non_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression\n",
    "\n",
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "\n",
    "model = regression.fit(features_polynomial, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00632])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first observation of the dataset\n",
    "\n",
    "features_non_linear[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a polynomial feature we would raise the first observantion's value to the second degree x² \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.99424e-05])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first observation raised to the second power x²\n",
    "\n",
    "features_non_linear[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would be our new feature, we will then also raise the value to x³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.52435968e-07])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first observation raised to the third power x³ \n",
    "\n",
    "features_non_linear[0]**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including all three features in our features matrix and then running a linear regression we have conducted a polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.32000000e-03, 3.99424000e-05, 2.52435968e-07])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first observation values for x, x²,x³\n",
    "\n",
    "features_polynomial[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance with Regularization \n",
    "\n",
    "Use a learning algorithm that includes a shrinkage penalty (regularization) like ridge regression and lasso regression. They are different because of they apply different shrinkage penaltys. Ridge VS Lasso == Better predictions VS More interpretable answer\n",
    "\n",
    "In standard linear regression the model trains to minimize the Sum of Squared Errors between the true and prediction y-ŷ.\n",
    "\n",
    "Regularized regression learners are similar but they apply a shrinkage penalty that actually, makes the model shrink\n",
    "\n",
    "**Elastic Net** Simple regresssion model with both penalties included.\n",
    "\n",
    "Regardless of which one to use, both ridge and lasso regression can penalize large or complex models by including coefficients values in the loss function we are trying to minimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression \n",
    "\n",
    "The shrinkage penalty is a tuning hyperparameter multiplied by the squared sum of all coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "features_ridge = boston.data\n",
    "target_ridge = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ridge Regression with an alpha value\n",
    "\n",
    "regression = Ridge(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.92396151,  1.07393055,  0.12895159,  0.68346136, -2.0427575 ,\n",
       "        2.67854971,  0.01627328, -3.09063352,  2.62636926, -2.04312573,\n",
       "       -2.05646414,  0.8490591 , -3.73711409])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the linear Regression \n",
    "\n",
    "model = regression.fit(features_standardized, target)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter alpha let us control how much we penalize the coefficients with higher values of alpha creating simpler models.\n",
    "\n",
    "The ideal value of alpha should be tuned like any other hyperparameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include a RidgeCV method that allows us to select the idal value of alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ridge Regression with three alpha values\n",
    "\n",
    "regression_ridgecv = RidgeCV(alphas = [0.1, 1.0, 10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "\n",
    "model_cv = regression_ridgecv.fit(features_standarized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.91987132,  1.06646104,  0.11738487,  0.68512693, -2.02901013,\n",
       "        2.68275376,  0.01315848, -3.07733968,  2.59153764, -2.0105579 ,\n",
       "       -2.05238455,  0.84884839, -3.73066646])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Coefficients\n",
    "\n",
    "model_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see easily the best modelś alpha value.\n",
    "\n",
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "Simplify your linear regression model by reducing the number of features.\n",
    "\n",
    "The shrinkage penalty is a tuning hyperparameter multiplied by the sum of the absolute value of all coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries \n",
    "\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lasso Regression \n",
    "\n",
    "regression_lasso = Lasso(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the linear regression\n",
    "\n",
    "model_lasso = regression_lasso.fit(features_standarized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can shrink model to 0. Effectively reducing the number of feartures in the model. For example, in our solution we set alpha to 0.5 and we can see that many coefficients are 0, meaning that their corresponding features are not used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11526463,  0.        , -0.        ,  0.39707879, -0.        ,\n",
       "        2.97425861, -0.        , -0.17056942, -0.        , -0.        ,\n",
       "       -1.59844856,  0.54313871, -3.66614361])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View coefficients\n",
    "\n",
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we increase alpha to a much higher value we can see that literally none of the features are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.,  0., -0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Lasso alpha = 10 \n",
    "\n",
    "regression_a10 = Lasso(alpha=10)\n",
    "model_a10 = regression_a10.fit(features_standardized, target)\n",
    "model_a10.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The practical benefit of this effect is taht it means that we could include 100 features in our feature matrix and then, through adjusting lasso's alpha hyperparameter produce a model that uses only 10 of the most important features.\n",
    "\n",
    "This let us reduce variance while improving the interpretability of our model. (fewer features are easier to explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
