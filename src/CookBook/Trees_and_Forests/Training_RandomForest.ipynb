{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Random Forest Classifier\n",
    "\n",
    "You want to train a classification model using a forest of randomized decision trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier object\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state = 0, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "model = randomforest.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new observation\n",
    "\n",
    "observation = [[5,4,3,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict observation's class\n",
    "\n",
    "model.predict(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common Problem with decision trees is that they tend to fit the training data too closely (i.e., overfitting) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change measuse of dplit quality used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classidier object using entropy\n",
    "\n",
    "randomforest_entropy = RandomForestClassifier(criterion=\"entropy\", random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model \n",
    "\n",
    "model_entropy = randomforest_entropy.fit(features, target)\n",
    "model_entropy.predict(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_features: By default is set to auto ( = sqrt(p)). Where p are the total number of features. \n",
    "\n",
    "It determines the maximun features to be considered at each node and takes a number of arguments including integers, floats and sqrt\n",
    "\n",
    "Bootstrap : Allows us to set whether the subset of observations considered for a tree is created using sampling with replacement. default = True\n",
    "\n",
    "n_estimators : numbers of trees to construct. default = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig Random Forest Regressor\n",
    "\n",
    "You want to train a regression model using \"forest\" of randomized decision trees.\n",
    "\n",
    "Each tree will use a bootstrapped subset of observations and at each node the decision rule considers only a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "# Create features matrix and target vector\n",
    "\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest Regressor\n",
    "\n",
    "randomforest = RandomForestRegressor(random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = randomforest.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifyinf Important Features in Random Forest\n",
    "\n",
    "You need to know which features are most important in a random forest model.\n",
    "\n",
    "One of the major benefits of decision trees is intrepretability. Specifically we can visualize the entire model. \n",
    "\n",
    "We can compare and visualize the relative importance of each feature.\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "Scikit learn requires that we break up nominal categorical features into multiple binary features. This has the effect of spreading the importance of that feature across all of the binary features and can often make each feature appear to be unimportant\n",
    "\n",
    "If two features are highly correlated one feature will claim much of the importance, making the other feature appear to be far less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier object\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = randomforest.fit(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn, classification and regression decision trees and random forest can report the relative importance of each feature.\n",
    "The higher the number the higher the importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09090795, 0.02453104, 0.46044474, 0.42411627])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importante of features calculated\n",
    "\n",
    "importances = model.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort feature importances in descending order\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['petal length (cm)',\n",
       " 'petal width (cm)',\n",
       " 'sepal length (cm)',\n",
       " 'sepal width (cm)']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Rearrange feature names so they match the sorted featur importances\n",
    "\n",
    "names = [iris.feature_names[i] for i in indices]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x7f70289495f8>,\n",
       "  <matplotlib.axis.XTick at 0x7f7028927d30>,\n",
       "  <matplotlib.axis.XTick at 0x7f70289276a0>,\n",
       "  <matplotlib.axis.XTick at 0x7f70288f3278>],\n",
       " <a list of 4 Text xticklabel objects>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Plot\n",
    "\n",
    "plt.figure(facecolor = \"orange\")\n",
    "\n",
    "# Create Plot Title\n",
    "\n",
    "plt.title(\"Features importance\")\n",
    "\n",
    "# Add bars\n",
    "\n",
    "plt.bar(range(features.shape[1]), importances[indices])\n",
    "\n",
    "# Add feature names as x-acis labels\n",
    "\n",
    "plt.xticks(range(features.shape[1]), names, rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Important Features in Random Forest\n",
    "\n",
    "Conduct feature selection on a random forest.\n",
    "\n",
    "Identify the importance features and retrain the model using only the most important features.\n",
    "\n",
    "There are situations where we might want to reduce the number of features , for instance : \n",
    "\n",
    "To reduce the model's variance\n",
    "\n",
    "Improve interpretability of the model by considering only the most important features.\n",
    "\n",
    "We can create a simple two-stage workflow to create a model with reduced features.\n",
    "\n",
    "First we train a random forest model using all features. \n",
    "\n",
    "Then we use this model to identify the most important features. \n",
    "\n",
    "Next we create a feature matrix that includes only these features. \n",
    "\n",
    "The we use SelectFromModel method to create a feature matrix containing only features with an importance greater than or equal to a threshold value (0.3) .\n",
    "\n",
    "Finally we create a model containing only those values.\n",
    "\n",
    "\n",
    "\n",
    "### Caveats \n",
    "\n",
    "In this approach there are two main things to take into consideration:\n",
    "\n",
    "- Nominal categorical features that have been one hot encoded will see the feature importance siluted across the binary features.\n",
    "- The feature importance of highly correlated features will be effectively assigned to one feature and not evenly distributed across both features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier\n",
    "\n",
    "randomforest = RandomForestClassifier (random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object that selects features with importance greater than or equal to a threshold\n",
    "\n",
    "\n",
    "selector = SelectFromModel(randomforest, threshold = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature matrix using selector\n",
    "\n",
    "features_important = selector.fit_transform(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Using most Important Features\n",
    "\n",
    "model = randomforest.fit(features_important,target) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Classes\n",
    "\n",
    "You have a target vector with highly imbalanced classes and want to train a random forest model.\n",
    "\n",
    "## Imbalanced clasess\n",
    "\n",
    "Very common in real world, can reduce the performance of our model. \n",
    "\n",
    "RandomForestClassifier(class_weight = {\"male\" = 0.2 , \"female\" = 0.8} will weight the classes accordingly\n",
    "\n",
    "class_weight = \"balanced\" ==> Useful argument whereinn classes are automatically weighted inversely proportinal to how frequently thy appear on the data\n",
    "\n",
    " Wj = N / Knj \n",
    " \n",
    " k = Number of clases = 2\n",
    " n = observations == 110\n",
    " 10 /100 observations of each class \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weight \n",
    "\n",
    "print(\"Calculate weight for small class ==> 110 / (2*10) = \", 110 / (2*10))\n",
    "print(\"Calculate weight for large class ==> 110 / (2*100) = \"  , 110 / (2*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make class highly imbalanced by removing first 40 observations\n",
    "\n",
    "features = features[40:,:]\n",
    "target = target[40:]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create target vector indicating if class 0 otherwise 1\n",
    "\n",
    "target = np.where((target == 0), 0 , 1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier object\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state = 0, n_jobs = -1, class_weight = \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model\n",
    "\n",
    "model = randomforest.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling Tree Size \n",
    "\n",
    "You want to manually determine the structure and sizr of a decision tree.\n",
    "\n",
    "max_depth == deep of tree.If None the tree is grown until all leaves are pure. If an integer, 3 id effectively \"pruned\" to that depth\n",
    "\n",
    "min_samples_split == Minimum number of observations at a node before that node is split. if integet, raw minimum. if float is the percent of observations\n",
    "\n",
    "min_samples_leaf == minimum number of observations required to be at a leaf. (same arguments than min_samples_split)\n",
    "\n",
    "max_leaf_nodes == maximum number of leaves\n",
    "\n",
    "min_impurity_decrease == Minimum impurity decrease required before a splot is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create decision tree classifier objet\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state = 0,\n",
    "                                      max_depth = None,\n",
    "                                      min_samples_split = 2,\n",
    "                                      min_samples_leaf = 1,\n",
    "                                      min_weight_fraction_leaf = 0,\n",
    "                                      max_leaf_nodes = None,\n",
    "                                      min_impurity_decrease = 0)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = decisiontree.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance through boosting\n",
    "\n",
    "You need a model with better performance than decision trees or random forests.\n",
    "\n",
    "## Adaboost\n",
    "\n",
    "Approach boosting form. Train a series of weak models (shallow decision tree, stump) , each interaction giving higher priority to observaitons the previous model predicted incorrectly.\n",
    "\n",
    "Assigns every observation an initial weight value. w = 1 / n . n is the total number of observations in the data.\n",
    "\n",
    "Train a weak model on the data\n",
    "\n",
    "For each observation : \n",
    "\n",
    "- if weak model predicts correctly w is increased\n",
    "- if weak model predicts incorrectly w is decreased\n",
    "\n",
    "Train a new weak model where observations with greater w are given greater priority\n",
    "\n",
    "Repeat steps un til data is perfectly predicted or a preset number of weak models has been trained.\n",
    "\n",
    "The end result is an aggregated model where individuals weak models focus on more difficult observations. Parameters : \n",
    "\n",
    "- base_estimator : learning algorithm to train weak models. decision tree by default\n",
    "- n_estimators : number of models to iteratively train\n",
    "- learning_rate : contribution of each model to the weights. 1 by default. \n",
    "- loss : exclusive from AdaBoostRegressor, set the loss function to use when updating weights. linear function by default, could be exponential or square.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=50, random_state=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create adaboost Classifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
    "                   n_estimators=50, random_state=0)\n",
    "adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = adaboost.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Random Forest With Out of Bags Errors\n",
    "\n",
    "Validating Random FOrest wthout using cross-validation.\n",
    "\n",
    "\n",
    "## Out of Bags Errors\n",
    "\n",
    "In random forest each decision tree is trained using a bootstrapped subset of observations. This means that for every tree there is a separate subset of observations not being used to train that tree, these are called OOB, out of bags observations. \n",
    "\n",
    "We can use OOB observations as a test set ti evaluate the performance of our model.\n",
    "\n",
    "For wvery observation the algorithm compares the observation's true values with the prediction from a subset of trees not trained using that observations. The overall score is calculated and provides asingle measure of a random forest performance. \n",
    "\n",
    "Excellent alternative to Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest classifier\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state = 0, n_estimators = 1000, oob_score = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model \n",
    "\n",
    "model = randomforest.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View out of bag error\n",
    "\n",
    "randomforest.oob_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
