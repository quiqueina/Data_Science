{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Despite called Regression, actually, Logistic Regression is a widely used supervised classification technique.\n",
    "\n",
    "Allow us to predict the probability that an observation is of a certain class using straightforward and well understood approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Binary Classifier\n",
    "\n",
    "Train a simple classifier model.\n",
    "\n",
    "The target vector can only take two values.\n",
    "\n",
    "In Logistic Regression, a linear model is included in a logistic function (also called sigmoid).\n",
    "\n",
    "P(yi = 1|X) ==  1 / [1+(e)^-(Bo+B1x)]\n",
    "\n",
    "___\n",
    "\n",
    "P(yi = 1|X) :probability of the ith observation's target value yi\n",
    "\n",
    "X : is the training data\n",
    "\n",
    "Bo, B1 : parameters to be learned\n",
    "\n",
    "e : Euler's Number\n",
    "\n",
    "---\n",
    "\n",
    "The effect of the logistic function is to contrain the value of the function's output between 0 and 1 so that it can be interpreted as a probability.\n",
    "\n",
    "If P(yi = 1|X) is greeater than 0.5 class 1 is predicted, otherwise class 0 is predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data[:100,:] #shape = (100,4)\n",
    "target = iris.target[:100]   #shape = (100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "features_standardized = standardizer.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create logistic regression object\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state = 0)\n",
    "logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model \n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation\n",
    "\n",
    "observation = [[0.5, 0.5, 0.5, 0.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict class\n",
    "\n",
    "model.predict(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17738424, 0.82261576]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View predicted probabilities\n",
    "\n",
    "model.predict_proba(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our observation had an 17,73% chance of being class 0 and 82,26 %of being class 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Our observation had an 17,73% chance of being class 0 and 82,26 %of being class 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Multiclass Classifier\n",
    "\n",
    "Given more than two classes you need to train a model classifier.\n",
    "\n",
    "### One_VS_Rest / Multinomial methods\n",
    "\n",
    "On their own, logistic regression are only binary classifier, meaning they cannot handle target vector with more than two classes. \n",
    "\n",
    "OneVSRest (OVR) : A separated model is trained for each class predicted whether an observation is that class or not (making a binary classification problem)(class 0 or not)  . It assumes that each classification problem is independent.\n",
    "\n",
    "Multinomial Logistic Regression (MLR) : The logistic function is replaced with a softmax function. One advantage is that MLR predicts probabilities using predict_proba  which is better calibrated\n",
    "\n",
    "the argumentmulti_class can be changed depending on our decision.\n",
    "\n",
    "multi_class= \"multinomial\" / \"ovr\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Standardizer\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OneVSRest Logisctic Regression object \n",
    "\n",
    "logistic_regression = LogisticRegression(random_state = 0, multi_class = \"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Variance Through Regularization\n",
    "\n",
    "You need to reduce the variance of your logistic regression model.\n",
    "\n",
    "Tune the strenght hyperparameter, C. Scikit learn use C as the inverse of the parameter alpha C = 1 / alpha\n",
    "\n",
    "### Regularization : \n",
    "\n",
    "is a method of penalizing complex models to reduce their variance. Specifically, a penalty term is added to the loss function we are trying to minimize typically the L1 and L2 penalties.\n",
    "\n",
    "L1 Penalty : alpha * E|Bj|\n",
    "\n",
    "Bj is the parameters of the jth of p features being learned and alpha is a hyperparameter denoting the regularization strength. \n",
    "\n",
    "L2 Penalty : alpha * BÂ²j\n",
    "\n",
    "Higher values of alpha increase the penalty for larger parameter values. (more complex models)\n",
    "\n",
    "We can use C as a hyperparameter to be tuned to find the value of class to efficiently tune C.\n",
    "\n",
    "#### Cs\n",
    "\n",
    "Cs parameter can either accept a range of values for C to search over ( if a list of floats is supplied as an argument).\n",
    "\n",
    "Will generate a list of that many candidate values drawn from a logarithmic scale between (-10000, 10000.\n",
    "\n",
    "it would be a good idea to apply also model selection to the penalty term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier object\n",
    "\n",
    "logistic_regression = LogisticRegressionCV(penalty=\"l2\", Cs = 10, random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifier on very large data\n",
    "\n",
    "**SOLVER** Stochastic Average Gradient (SAG)\n",
    "\n",
    "Scikit learn will select the best solver automatically for us or warn us that we cannot do something with that solver.\n",
    "\n",
    "It allows us to train a model much faster than others solvers when our data is very large.\n",
    "\n",
    "It is also a very sensitive to feature scaling so it is very important to standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier object\n",
    "\n",
    "logistic_regression = LogisticRegressionCV(max_iter = 10000, solver = \"sag\", random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Classes\n",
    "\n",
    "Training a simple classifier model.\n",
    "\n",
    "If we have highly imbalanced classes and have not adressed it during the preprocessing we have the option of using class_weight parameter to weight the classes to make certain we have a balanced mix of each class.\n",
    "\n",
    "class_weight = \"balanced\" ==> Will automatically weigh classes inversely proportional to their frequency. w = n / knj\n",
    "\n",
    "w : weight to class \n",
    "\n",
    "n: number of observations\n",
    "\n",
    "k: total number of classes\n",
    "\n",
    "nj: number of observations in class j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make class highly imbalanced by removing first 40 observations\n",
    "\n",
    "features = features[40:,:] #shape = (110,4)\n",
    "target = target[40:]      # shape = (110,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create target vector indicating if class 0, otherwise 1\n",
    "\n",
    "target = np.where((target == 0),0,1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier object\n",
    "\n",
    "logistic_regression = LogisticRegressionCV(max_iter = 10000, class_weight = \"balanced\", random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
