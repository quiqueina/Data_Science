{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Bayes theorem is the premier method for understanding the probability of some event P(A|B) given some new information, P(B|A) and a prior belief in the probability event.\n",
    "\n",
    "P(A|B) = [ P(B|A) * P(A) ]  /  P(B)\n",
    "\n",
    "Qualities of Naive Bayes Classifiers:\n",
    "\n",
    "- Intuitive approach\n",
    "- The ability to work with small data\n",
    "- Low computation cost for training and prediction\n",
    "- Often solid results in a variety of settings\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    " **FORMULA : P(y|x1, ..., xj) = [P(xi, ..., xj | y) . P(y)] / P(x1, ... , xj)**\n",
    "\n",
    "P(y|x1, ..., xj) : POSTERIOR. Probability that an observation is class y given the observation's values for the j features x1, ..., xj\n",
    "\n",
    "P(xi, ..., xj | y) : LIKELIHOOD. Likelyhood of an observation's values for features x1, ..., xj given their class y\n",
    "\n",
    "P(y) : PRIOR. Our belief for the probability of class y bedore looking at the data\n",
    "\n",
    "P(x1, ... , xj) MARGINAL PROBABILITY\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In naive bayes we compare an observation's posterior values for each possible class.\n",
    "\n",
    "The marginal probability is constant across this comparisons.\n",
    "\n",
    "We calculate the numerators of the posterior for each class. \n",
    "\n",
    "For each operation the class with the greatest posterior numerator becomes the predicted class Å·\n",
    "\n",
    "---\n",
    "\n",
    "1 -. In Naive Bayes for each feature in the data we have to assume the statistical distribution of the likelihood P(Xj|y)\n",
    "\n",
    "The common distributions are : Normal(gaussian) ; Multinomial ; Bernoulli\n",
    "\n",
    "The distribution chosen is oftenly determinided by the nature of features ( continous, binary, etc ... ) \n",
    "\n",
    "2-. Naive Bayes. Get it names because we assume that each feature and its resulting likelihood is independent. \n",
    "\n",
    "This \"naive\" assumptions os frequently wrong, yet in practice does little to prevent bulding high quality classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier for Continuous features\n",
    "\n",
    "You have only continous features and you want to train GAUSSIAN Naive Bayes classifier.\n",
    "\n",
    "It assumes that the likelihood of the feature values, x, given an observation is of class y, folows a Normal Distribution. N~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gaussian naive Bayes Object\n",
    "\n",
    "classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "model = classifier.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation \n",
    "\n",
    "observation = [[4, 4, 4, 0.4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict observation class\n",
    "\n",
    "model.predict(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gaussian Naive ayes object with prior probabilities of each class\n",
    "\n",
    "clf = GaussianNB(priors = [0.25, 0.25, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "model = classifier.fit(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.predict_proba_ is not working weel calibrated, we will need to calibrate them using an isotonic regression or a related method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Classifier for Discrete and Count Features.\n",
    "\n",
    "Given discrete or count data you need to train a naive Bayes Classifier.\n",
    "\n",
    "### Multinomial Naive Bayes Classifier\n",
    "\n",
    "Works similar to Gaussian NB but the features are assumed to be multinomially distributed.\n",
    "\n",
    "This means that this classifier is commonly used when we have discrete data.\n",
    "\n",
    "Bag of words or tf-idf approaches.\n",
    "\n",
    "We have created a toy text dataset of three observations, and converted the text strings sto a bag-of-words feature matrix and an acoompanying target vector.\n",
    "\n",
    "We then used MultinomialNB to train a model while defining the prior probabilities for the two classes. (Pro Granada VS Pro Zanzibar)\n",
    "\n",
    "is class_prior is noy specified, prior_probabilities are learned using the data. However if we want a uniform distribution to be used as te prior we can set fit_prior = False.\n",
    "\n",
    "alpha = smoothing parameter  that should be tunned (0.0,1.0) meaning no smoothing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text\n",
    "\n",
    "text_data = np.array([ \" I love Granada, Granada is awesome!\", \"Granada is the best\", \"Zanzibar is also great\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bag-of-words\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'awesome', 'best', 'granada', 'great', 'is', 'love', 'the', 'zanzibar']\n",
      "[[0 1 0 2 0 1 1 0 0]\n",
      " [0 0 1 1 0 1 0 1 0]\n",
      " [1 0 0 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix\n",
    "\n",
    "features = bag_of_words.toarray()\n",
    "print(count.get_feature_names())\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target vector \n",
    "\n",
    "target = np.array([0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multinomial naive Bayes object with prior probabilities of each class\n",
    "\n",
    "classifier = MultinomialNB(class_prior = [0.25, 0.5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "model = classifier.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation\n",
    "\n",
    "new_observation = [[0, 0, 0, 1, 0, 1, 0, 1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict new observation's class\n",
    "\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Naive Bayes Classifier for Binary Features\n",
    "\n",
    "Bernoulli Naive Bayes Classifier Assumes that all features are binary, two values. \n",
    "\n",
    "BernoulliNB:\n",
    "\n",
    "It is often used in text classification, when our feature matrix is the presence/absence of a word in a document.\n",
    "\n",
    "class_prior : True return a list of prior probabilities for each class.  False = uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create three binary features \n",
    "\n",
    "features = np.random.randint(2, size = (100,3)) # two classes , 100 rows, 3 features\n",
    "features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create binary target vector \n",
    "\n",
    "target = np.random.randint(2, size = (100,1)).ravel()\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bernoulli NB object with prior probabilities of each  class \n",
    "\n",
    "classifier = BernoulliNB(class_prior = [0.25, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=[0.25, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = classifier.fit(features, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibrating Predicted Probabilities\n",
    "\n",
    "Calibrate the predicted probabilities from naive bayes classifier so they are interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "features=iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Gaussian Naive Bayes object\n",
    "\n",
    "classifier_gaussian = GaussianNB()\n",
    "classifier_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibrated cross validation with sigmoid calibration\n",
    "\n",
    "classifier_sigmoid = CalibratedClassifierCV(classifier_gaussian, cv= 2, method = \"sigmoid\")\n",
    "\n",
    "# method = sigmoid / isotonic regression (nonparametric, it tend to overfit when sample size are vert small (i.e: 100 observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalibratedClassifierCV(base_estimator=GaussianNB(priors=None,\n",
       "                                                 var_smoothing=1e-09),\n",
       "                       cv=2, method='sigmoid')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "model_sigmoid = classifier_sigmoid.fit(features, target)\n",
    "model_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation \n",
    "\n",
    "observation_new = [[2.6, 2.6, 2.6, 0.4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View calibrated probabilities\n",
    "\n",
    "model_sigmoid.predict_proba(observation_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict_proba is extremely useful when predicting a certain class if the model predicts the probability of being that class over 90%.\n",
    "\n",
    "Naive Bayes often output probabilities that are no based on the real world. \n",
    "\n",
    "While the ranking of predicted probabilitie for the different target classes is valid, the raw predicted probabilities tent to take on extreme values 0,1\n",
    "\n",
    "To obtain meaningful predicted probabilities we need to conduct what is called calibration.\n",
    "\n",
    "### Calibration Classifier CV\n",
    " \n",
    "Use Croos validation\n",
    "\n",
    "We can see the difference between the raw and well-calibrated predicted probabilities. \n",
    "\n",
    "Using a GaussianNB Classifier we can see very extreme probability estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.31548432e-04, 9.99768128e-01, 3.23532277e-07]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a GaussianNB, then predict probabilities\n",
    "\n",
    "classifier_gaussian.fit(features,target).predict_proba(observation_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31859969, 0.63663466, 0.04476565]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sigmoid.predict_proba(observation_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
