{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "**How do they work ?**\n",
    "\n",
    "A neuron or node is the unit that takes in one or more inputs, multiplies each input by a parameter (weight), sums the weighted input's values along with some bias value (tipically 1) and then feeds the value into an activation function.\n",
    "\n",
    "Feedforward neural network : Also called multilayer perceptron is the simplest artificial network.\n",
    "\n",
    "The name is obtained from the fact that an observation's feature values are fed forward through network, with each layer successively transforming the feature values with the goal that the output at the end is the same as the target's value.\n",
    "\n",
    "- Input layer : Each unit contains an observation's value for a single feature. (100 features = 100 nodes)\n",
    "- Output layer : End of neural network.Transform the output of the hidden layer into values useful for the task at hand.\n",
    "- Hidden layers: Between input and output layers. Transform the features values from the input layer to do something that once processed resembles the target class.\n",
    "\n",
    "(When having hidden layers is called DeepLearning)\n",
    "\n",
    "\n",
    "Forward Propagation : Parameters are initialize as small random values from a gaussian or normal uniform. Once an observation is fed through the network, the outputted value is compared with the observation's true value using a loss function.\n",
    "\n",
    "Backpropagation : An algorithm that goes backward though the network identifying how much each parameter contributed to the error between predicted and true values. At each parameter the optimization algorithm determines how much each weightt should be adjusted to improve the output.\n",
    "\n",
    "Neural network learn by repeating the process of forward propagation and backpropagation for every observation in the training data multiple times.\n",
    "\n",
    "Epoch: Each time all observations have been sent through the network, training consists on multiple epochs iteratively updating the values of the parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data for Neural Networks\n",
    "\n",
    "Use standard scaler.\n",
    "\n",
    "It is very important for neural networks.\n",
    "\n",
    "NN often behave poorly when features values are much larger than parameter values. Furthermore, since an observation's feature values are combined as they pass thruogh individuals units. IT IS VERY IMPORTANT THAT ALL THE FEATURES ARE THE SAME SCALE,\n",
    "\n",
    "Choose an architecture of the NN is an art.\n",
    "\n",
    "Construct FeedForward NN : \n",
    "\n",
    "- 1-. Receives a number of inputs\n",
    "- 2-. Weights each input by a parameter value.\n",
    "- 3-. Sums together all weighted inputs along with some bias (tipically 1)\n",
    "- 4-. Apply activation function\n",
    "- 5-. Send the output in the next layer \n",
    "\n",
    "For each layer in the hidden and output layers we must define the number of units to inclide in the layer and the activation function.\n",
    "\n",
    "The more units we have in a layer the more oir network is able to learn complex patterns.\n",
    "\n",
    "The more units can make our model overfit the training data in a way detrimentalto the performance of the test data. \n",
    "\n",
    "RELU : Rectified Linear Unit. Activation function. f(z) = max(o,Z) where X is the sum of weighted inputs and bias.\n",
    "\n",
    "We need to fefine the number of hidden layers to use in the network. More layers allow the network to learn more complex relationships.\n",
    "\n",
    "Er have to define the structure of the activation function of he output layer.\n",
    "\n",
    "Output layers patterns : \n",
    "\n",
    "- Binary classification: one unit with a sigmoid activation fucntion\n",
    "- Multiclass classification : k units (k = number of classes) and a softmax activation fucntion\n",
    "- Regression : One unit with no activation function\n",
    "\n",
    "Loss Function : \n",
    "\n",
    "- Binary classification : Binary cross entropy\n",
    "- Multiclass classification : Categorical cross entropy\n",
    "- Regression : Mean Squared Error\n",
    "\n",
    "Determine optimizer : \n",
    "\n",
    "Walking around, tbe loss function to find the parameter that produce the lowest error.\n",
    "Common choices : stochastic gradient descend with/without momentum, root mena squared propagation, adaptative moment estimation.\n",
    "\n",
    "We can select one or more metrics to evaluate the performance such as Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-100.1, 3240.1],\n",
       "       [-200.2, -234.1],\n",
       "       [5000.5,  150.1],\n",
       "       [6000.6, -125.1],\n",
       "       [9000.9, -673.1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature\n",
    "\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                     [-200.2, -234.1],\n",
    "                     [5000.5, 150.1],\n",
    "                     [6000.6, -125.1],\n",
    "                     [9000.9, -673.1]])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the feature\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.0\n",
      "Standard Deviation:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \", round(features_standardized[:,0].mean()))\n",
    "print(\"Standard Deviation: \", round(features_standardized[:,0].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Neural Network\n",
    "\n",
    "Using keras sequential model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural Network\n",
    "\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a ReLU activation function\n",
    "\n",
    "network.add(layers.Dense(units = 16, activation = \"relu\", input_shape=(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a ReLU activation function\n",
    "\n",
    "network.add(layers.Dense(units = 16, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connected layer with a sigmoid activation function\n",
    "\n",
    "network.add(layers.Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Neural network\n",
    "\n",
    "network.compile(loss = \"binary_crossentropy\",\n",
    "                optimizer = \"rmsprop\",\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Binary Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of features we want\n",
    "\n",
    "number_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load data and target vector frin nivue review data\n",
    "\n",
    "(data_train, target_train),(data_test, target_test) = imdb.load_data(num_words = number_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert movie review data to one-hot encoded feature matrix\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_features)\n",
    "\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode= \"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Neural Network\n",
    "\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully conected layer with ReLu activation function\n",
    "\n",
    "network.add(layers.Dense(units = 16,activation = \"relu\", input_shape = (number_features,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add fully connectec layer with ReLU activation fucntion\n",
    "\n",
    "network.add(layers.Dense(units = 16, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dully connected layer with a sigmoid activation\n",
    "\n",
    "network.add(layers.Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network \n",
    "\n",
    "network.compile(loss = \"binary_crossentropy\", optimizer = \"rmsprop\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kike/Documentos/repos/Data_Science/kikenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 2s 70us/step - loss: 0.4096 - accuracy: 0.8180 - val_loss: 0.3367 - val_accuracy: 0.8558\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 53us/step - loss: 0.3251 - accuracy: 0.8636 - val_loss: 0.3252 - val_accuracy: 0.8621\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.3134 - accuracy: 0.8684 - val_loss: 0.3462 - val_accuracy: 0.8498\n"
     ]
    }
   ],
   "source": [
    "# Train a Neural Network\n",
    "\n",
    "history = network.fit(features_train, target_train, epochs = 3, verbose = 1, batch_size = 100, validation_data = (features_test, target_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 50.000 movies reviews (50-50 training-test) categorized as positives or negatives.\n",
    "\n",
    "We convert the text of the review in to 5000 binary features indicating the presence of one of the 1000 most frequent words. (25.000 observations with 1000 features to predict if a movie review is positive or negative)\n",
    "\n",
    "There are  6 parameter in fit method\n",
    "\n",
    "- 1-. features\n",
    "- 2-. target vector\n",
    "- 3-. epochs\n",
    "- 4-. verbose\n",
    "- 5-. batch_size (number of observations to propagate through the network before updating the parameters)\n",
    "- 6-. held out a test set of fata to use to evaluate the model. validation_data = test features and target vector can be arguments.\n",
    "- + validation_split (how to split data for evaluation) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
