{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction Using Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection** Selecting hight-quality, informative features and dropping less useful features. There are three type of feature selection: \n",
    "\n",
    "Filter : Select the best features by examining their statistical properties.\n",
    "\n",
    "Wrapper : Use trial and error to find the subset of features that produce models with the highest quality prediction.\n",
    "\n",
    "Embedded : Select the best feature subset as part or as an extension of a learning algorithm's training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding Numerical Feature Variance\n",
    "\n",
    "You have a set of numerical features and want to remove those with low variance (i.e., Containing little information).\n",
    "\n",
    "Motivated by the idea that features with low variance are likely less interestinf (and useful) than features with hight variance, the first step will be to calculate the variance of each feature, secondly, dropping those features whose variance does not satisfy the selected threshold.\n",
    "\n",
    "Things to keep in mind: \n",
    "\n",
    "1-. The variance is not centered because it is in the square unit of the feature itself. It will not work with features in different units (i.e, time and money)\n",
    "\n",
    "2-. The variance tthreshold is selected manually on our own judgement or by using a model selecion technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and target\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Thresholder\n",
    "\n",
    "thresholder = VarianceThreshold(threshold=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hight variance feature matrix \n",
    "\n",
    "features_hight_variance = thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View hight variance feature matrix\n",
    "\n",
    "features_hight_variance[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View variances\n",
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if thefeatures have been standardized (mean = 0, variance = 1) variance thresholding will not work correctly. (OBVIOUSLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize feature matrix\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Calcalute the variance of each feature \n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding Binary feature variance\n",
    "\n",
    "You have a set of categorical features and want to remove those with low variance (containing fewer information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to select a subset of features with a bernoulli random variable vatiance above a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix with: \n",
    "\n",
    "#Feature 0 : 80% class 0\n",
    "#Feature 1 : 80% class 1 \n",
    "#Feature 2 : 60% class 0, 40 % class 1 \n",
    "\n",
    "features = [[0,1,0],[0,1,1],[0,1,0],[0,1,1],[1,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run threshold by variance \n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1- .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion : \n",
    "\n",
    "One strategy for selecting hightly informative categorical features is to examine their variances. Formula: Var(x) = p(1-p) where p is the proportion of observations of class 1. Therefore, by setting p we can remove features where the vast majority of observations are one class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Hightly Correlated Features\n",
    "\n",
    "We suspect that features on feature matrix are hightly correlated so that we need to check it out by using a Correlation Matrix. Consider dropping out one of the correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature matrix with to hightly correlated features\n",
    "\n",
    "features= np.array([[1,1,1],[2,2,0],[3,3,1],[4,4,0],[5,5,1],[6,6,0],[7,7,1],[8,7,0],[9,7,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature matrix into DataFrame\n",
    "df = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRELATION MATRIX : \n",
      "          0         1         2\n",
      "0  1.000000  0.976103  0.000000\n",
      "1  0.976103  1.000000  0.034503\n",
      "2  0.000000  0.034503  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Create Correlation Matrix\n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "print(\"CORRELATION MATRIX : \" ) \n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UPPER TRIANGLE : \n",
      "    0         1         2\n",
      "0 NaN  0.976103  0.000000\n",
      "1 NaN       NaN  0.034503\n",
      "2 NaN       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Look the upper triangle of the correlation matrix to identify pairs of hightly correlated features.\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "print(\"UPPER TRIANGLE : \" ) \n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATAFRAME: \n",
      "   0  2\n",
      "0  1  1\n",
      "1  2  0\n",
      "2  3  1\n",
      "3  4  0\n",
      "4  5  1\n"
     ]
    }
   ],
   "source": [
    "# Remove one correlated feature\n",
    "df_final = df.drop(df.columns[to_drop], axis=1).head()\n",
    "print(\"FINAL DATAFRAME: \")\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Irrelevant Features for Classification\n",
    "\n",
    "You have a categorical target vector and want to remove uninformative features.\n",
    "\n",
    "When facing categorical values a good solution is to calculate a chi-square statistic between each featre and the target vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chi Square Statistic** : Examines the independence of two categorical vectors. \n",
    "\n",
    "It represents the difference between the observed number of obervations in each class of a categorical feature and what we would expect if that feature was independent with the target vector (No relationship). \n",
    "\n",
    "By calculating Chi2 between feature and target vector we obtain ameasurement of the independence between the two. \n",
    "\n",
    "If the target is independent of the feature variable is irrelevant for our purposes because it contains no useful information for classification.\n",
    "On the other hand, if the two variables are dependent they likjely are very informative for training our model\n",
    "\n",
    "Can only be calculated between two categorical vectors and all the values need to be non negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical data by converting data to integers\n",
    "\n",
    "features = features.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-. We calculate the chi2 on each feature and the target vector.\n",
    "2-. SelectKBest provides features with best statistics (k = number of feature we want to keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select two features with highest chi-squared statistics\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features,target)\n",
    "features_kbest[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features : Shape[0]: 150Shape[1]: 4\n",
      "Reduced number of features : Shape[0]: 150Shape[1]: 2\n"
     ]
    }
   ],
   "source": [
    "# Show Result\n",
    "\n",
    "print(\"Original number of features : \" + \"Shape[0]: \" + str(features.shape[0]) + \"Shape[1]: \" + str(features.shape[1]))\n",
    "print(\"Reduced number of features : \" + \"Shape[0]: \" + str(features_kbest.shape[0]) + \"Shape[1]: \" + str(features_kbest.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA F-VALUE\n",
    "\n",
    "Having quantitative features we can calculate the ANOVA F-VALUE between each feature and the target vector.\n",
    "\n",
    "We can use f_classif to calculate the ANOVA F-Value with each feature and the target vector.\n",
    "\n",
    "F- Value is a mean comparison, it tell us if the mean for each group is significatly different (i.e., women VS men).\n",
    "\n",
    "Score mean for women Vs Score mean for men\n",
    "\n",
    "H0: Similar Mean\n",
    "H1: Different Mean\n",
    "\n",
    "Accept HO: Doesnt help to predict\n",
    "Accept H1: It is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two features with highest F-Values\n",
    "\n",
    "fvalue_selector = SelectKBest(f_classif, k = 2)\n",
    "features_kbest = fvalue_selector.fit_transform(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features : Shape[0]: 150Shape[1]: 4\n",
      "Reduced number of features : Shape[0]: 150Shape[1]: 2\n"
     ]
    }
   ],
   "source": [
    "# Show Result\n",
    "\n",
    "print(\"Original number of features : \" + \"Shape[0]: \" + str(features.shape[0]) + \"Shape[1]: \" + str(features.shape[1]))\n",
    "print(\"Reduced number of features : \" + \"Shape[0]: \" + str(features_kbest.shape[0]) + \"Shape[1]: \" + str(features_kbest.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCEPTIL SELECTION \n",
    "\n",
    "Instead of selecting a specific number of features we can also select the top n percent of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Library\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 75% of features with highest F-Values\n",
    "\n",
    "fvalue_selector = SelectPercentile(f_classif, percentile = 75) # Percentile 67 is the edge, 3 features detected\n",
    "features_kbest = fvalue_selector.fit_transform(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features : Shape[0]: 150Shape[1]: 4\n",
      "Reduced number of features : Shape[0]: 150Shape[1]: 3\n"
     ]
    }
   ],
   "source": [
    "# Show Result\n",
    "\n",
    "print(\"Original number of features : \" + \"Shape[0]: \" + str(features.shape[0]) + \"Shape[1]: \" + str(features.shape[1]))\n",
    "print(\"Reduced number of features : \" + \"Shape[0]: \" + str(features_kbest.shape[0]) + \"Shape[1]: \" + str(features_kbest.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECURSIVELY ELIMINATING FEATURES\n",
    "\n",
    "Select automatically the best features to keep.\n",
    "\n",
    "RFE: Recursive Failure Eliminator.\n",
    "\n",
    "RFECV: Conduct RFE using Cross Validation.\n",
    "\n",
    "Train a model, each time removing a feature until model performance become worse. The remaining features are the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE : Train a model that contains some parameters(weight coefficients) like linear regression or support vector machine repeteadly.\n",
    "The first time we train the model we include all the features, then,we find the feature with the smallest parameter (assuming data is standardize) and removing it because it is less important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress an annoying but harmless warning\n",
    "\n",
    "warnings.filterwarnings(action = \"ignore\", module = \"scipy\", message = \"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature matrix, target, vector and the true coefficients\n",
    "\n",
    "features, target = make_regression(n_samples = 10000, n_features = 100, n_informative = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression\n",
    "\n",
    "ols = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44765157,  0.49282359,  1.2183224 ,  1.01099846],\n",
       "       [-1.67831485, -0.24767319,  0.37487546,  1.27089437],\n",
       "       [ 1.63634543, -1.11006511,  1.13698511, -0.93085913],\n",
       "       ...,\n",
       "       [ 0.06514231, -0.1330187 , -0.81817928, -1.19043446],\n",
       "       [-0.29483262, -0.98992622, -0.50707802, -0.21564811],\n",
       "       [ 0.65336141,  1.21830774,  1.36632941,  0.11158976]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursively eliminate features\n",
    "\n",
    "rfecv = RFECV(estimator = ols, step = 1, scoring = \"neg_mean_squared_error\") # estimator = ols, SVM ; step = features deletedper time ; scoring : quality metric= \n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we have conducted RFE we can see the  number of features we shoud keep: \n",
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We cam añsp see which of those features we should keep: \n",
    "\n",
    "rfecv.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 82, 34, 43, 94, 39, 25, 65, 23,  1, 41,  6,  2, 76, 59, 53,  5,\n",
       "       36, 19, 14, 46, 57, 81, 31, 55, 72,  7,  1, 83, 20, 90, 74, 64, 67,\n",
       "       30, 37, 11, 35, 61, 50, 47, 17, 18, 40, 70, 44,  1,  8, 95, 97, 16,\n",
       "       79, 22,  4, 56, 28, 27, 73, 88, 12, 24, 51, 58, 26, 85, 21, 92, 77,\n",
       "       66, 87, 86, 33, 75, 29, 13, 10, 63, 49, 84, 48, 38, 68, 15, 32, 78,\n",
       "        9, 60, 54, 62, 71, 45,  1, 96, 52,  3, 91, 80, 89, 42, 93])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W3 can also viwe the ranking of the features: (1 to worst)\n",
    "\n",
    "rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
