{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Best Models from Multiple Learning\n",
    "\n",
    "You want to select the best model by seaching overa range of learning algorithms and their respective hyperparameters.\n",
    "\n",
    "Creating a disctionary of candidate learning algorithms and their hyperparameters.\n",
    "\n",
    "### Multiple Learning\n",
    "\n",
    "Include some learning algorithms as part of the search space. \n",
    "\n",
    "In this example there are two models with their own hyperparameters and we define their candidates values using the format classifier__[hyperparameter name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Random Seed \n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create Feature matrix and target vector\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'classifier': [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                      intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                      multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                      random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                      warm_start=False)],\n",
       "  'classifier__penalty': ['l1', 'l2'],\n",
       "  'classifier__C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "         5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "         3.59381366e+03, 1.00000000e+04])},\n",
       " {'classifier': [RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                          criterion='gini', max_depth=None, max_features='auto',\n",
       "                          max_leaf_nodes=None, max_samples=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_jobs=None, oob_score=False, random_state=None,\n",
       "                          verbose=0, warm_start=False)],\n",
       "  'classifier__n_estimators': [10, 100, 1000],\n",
       "  'classifier__max_features': [1, 2, 3]}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary  with candidate learning algorithms and their hyperparameters\n",
    "\n",
    "# Define the the set of possible values for regularization hyperparameter space, C, and potential types of regularization penalties, \"penalty\"\n",
    "\n",
    "search_space = [{\"classifier\": [LogisticRegression(max_iter = 1000, solver = \"liblinear\")],\n",
    "                 \"classifier__penalty\":[\"l1\", \"l2\"],\n",
    "                 \"classifier__C\": np.logspace(0,4,10)},\n",
    "               \n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10,100,1000],\n",
    "                 \"classifier__max_features\": [1,2,3]}]\n",
    "\n",
    "# Dictionary for Random Forest Hyperparameters\n",
    "\n",
    "search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('classifier',\n",
       "                                        RandomForestClassifier(bootstrap=True,\n",
       "                                                               ccp_alpha=0.0,\n",
       "                                                               class_weight=None,\n",
       "                                                               criterion='gini',\n",
       "                                                               max_depth=None,\n",
       "                                                               max_features='auto',\n",
       "                                                               max_leaf_nodes=None,\n",
       "                                                               max_samples=None,\n",
       "                                                               min_impurity_decrease=0.0,\n",
       "                                                               min_impurity_split=None,\n",
       "                                                               min_samples_leaf=1,\n",
       "                                                               min_samples_split=2,\n",
       "                                                               min_weight_fract...\n",
       "                                                                min_impurity_decrease=0.0,\n",
       "                                                                min_impurity_split=None,\n",
       "                                                                min_samples_leaf=1,\n",
       "                                                                min_samples_split=2,\n",
       "                                                                min_weight_fraction_leaf=0.0,\n",
       "                                                                n_estimators=100,\n",
       "                                                                n_jobs=None,\n",
       "                                                                oob_score=False,\n",
       "                                                                random_state=None,\n",
       "                                                                verbose=0,\n",
       "                                                                warm_start=False)],\n",
       "                          'classifier__max_features': [1, 2, 3],\n",
       "                          'classifier__n_estimators': [10, 100, 1000]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create GridSearch\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Grid Search\n",
    "\n",
    "best_model = gridsearch.fit(features,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the search is completed we can use best_estimator_ to view best model's learning algorithm and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model:  LogisticRegression(C=7.742636826811269, class_weight=None, dual=False,\n",
      "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
      "                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# See the hyperparameters of the best model\n",
    "\n",
    "print(\"Best Model: \", best_model.best_estimator_.get_params()[\"classifier\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to predict values just like any other scikit-learng model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Target Vector\n",
    "\n",
    "\n",
    "best_model.predict(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
