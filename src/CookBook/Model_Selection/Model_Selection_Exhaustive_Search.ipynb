{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "In machine learning algorithms training is used to learn the parameters of a  model by minimizing some loss function. \n",
    "\n",
    "Many learning algorithms also gave hyperparameters that must be defined outside of the learning process. For example, RandomForest are collections of decision trees (hence the word forest) that must be set prior to fitting. This is referred as HYPERPARAMETER TUNNING / HYPERPARAMETER OPTIMIZATION / MODEL SELECTION. \n",
    "\n",
    "We can also check which model has a better performance between Support Vector Machine Classifier & RandomForest for instance. \n",
    "\n",
    "Model selection  == Selecting the best learning algorithm and It's best hyperparameters. \n",
    "\n",
    "C == Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Best Models Using Exhaustive Search\n",
    "\n",
    "You want to select the best model  by searching over a range of hyperparameters using GridSearchCV.\n",
    "\n",
    "### GridSearchCV\n",
    "\n",
    "It's a brute force approach to model selection using cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create Feature matrix and target vector\n",
    "\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Logistic Regression as Learning Algorithm\n",
    "\n",
    "logistic = linear_model.LogisticRegression(max_iter=1000, solver = \"liblinear\")\n",
    "logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create range of candidate penalty hyperparameter values.\n",
    "# Two Possible values for regularization penalty.\n",
    "\n",
    "penalty = [\"l1\", \"l2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Range of candidate regularization hyperparameter values \n",
    "# We define 10 possible values of C.\n",
    "\n",
    "C = np.logspace(0,4,10)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "        5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "        3.59381366e+03, 1.00000000e+04]),\n",
       " 'penalty': ['l1', 'l2']}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary hyperparameter candidates. 2 Hyperparameters == C & Penalty\n",
    "\n",
    "hyperparameters = dict(C=C, penalty = penalty)\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best model\n",
    "\n",
    "For each combination of C and regularization penalty values we train a model that evaluate it using K Fold Cross Validation.\n",
    "\n",
    "10 possible values of C * 2 penalty values * 5 folds = 100 candidates models from which the best was selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRIDSEARCH\n",
    "\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose = 0) # FOLDS = 5 ; VERBOSE determines the message printed by the processing. [0,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=1000, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='liblinear',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04]),\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit grid search\n",
    "\n",
    "best_model = gridsearch.fit(features,target)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty:  l1\n",
      "Best C:  7.742636826811269\n"
     ]
    }
   ],
   "source": [
    "# See the hyperparameters of the best model\n",
    "\n",
    "print(\"Best Penalty: \", best_model.best_estimator_.get_params()[\"penalty\"])\n",
    "print(\"Best C: \", best_model.best_estimator_.get_params()[\"C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After identifying the best hyperparameters, GridSearch will retrain a model using the best hyperparameters on the entire dataset. We can use this model to predict values just like any other scikit-learng model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict Target Vector\n",
    "\n",
    "\n",
    "best_model.predict(features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
