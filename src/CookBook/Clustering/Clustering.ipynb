{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Situation where we only know the features and not the target (real world). UNSUPERVISED LEARNING\n",
    "\n",
    "The goal of clustering is to identify those talent groupings of observations, which is done well, allows us to predict the class of observations even without a target vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K - MEANS\n",
    "\n",
    "Splitting up into k groups.\n",
    "\n",
    "The algorithm attempts to group observations into k groups, having each group equal variance.\n",
    "\n",
    "K is selecter by the user as hyperparameter.\n",
    "\n",
    "1-. k cluster center points are created at random locations\n",
    "\n",
    "2-. For each observation : The distance between each observation and the k center points is calculated. + The observation assigned to the cluster of the nearest center point.\n",
    "\n",
    "3-. The center points are moved to the means (centers) of their respective cluster\n",
    "\n",
    "4-. steps2 and 3 are repeated until no observation changes in cluster membership\n",
    "\n",
    "The model is considered as converged and stop.\n",
    "\n",
    "\n",
    "K means assumes that:\n",
    "Clusters are convex shaped \n",
    "\n",
    "All features are equally scaled. (thats why we standardize the data)\n",
    "\n",
    "The groups are balanced.\n",
    "\n",
    "If we can't fulfil this requirements it would be a good idea to try another algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create K - Means object\n",
    "\n",
    "cluster = KMeans(n_clusters = 3, random_state = 0, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=3, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "       random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model = cluster.fit(features_standardized)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "       0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View predict class ( Predicted classes)\n",
    "\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare this to the observations true we can see despite the diffence in class labels , k-means did reasonably well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new observation\n",
    "\n",
    "new_observation = [[0.8, 0.8, 0.8, 0.8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict observation class\n",
    "\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.13597027,  0.08842168,  0.99615451,  1.01752612],\n",
       "       [-1.01457897,  0.85326268, -1.30498732, -1.25489349],\n",
       "       [-0.05021989, -0.88337647,  0.34773781,  0.2815273 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View clusters centers\n",
    "\n",
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette measure the similarit of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding Up K-Means using Minibatch\n",
    "\n",
    "If k-means takes too long this reduces the time required with an small cost in quality.\n",
    "\n",
    "Works similarly to Kmeans, batch-size determines the time required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = MiniBatchKMeans(n_clusters = 3, random_state = 0, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "                init_size=None, max_iter=100, max_no_improvement=10,\n",
       "                n_clusters=3, n_init=3, random_state=0, reassignment_ratio=0.01,\n",
       "                tol=0.0, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = cluster.fit(features_standardized)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using Meanshift\n",
    "\n",
    "Group observation without assuming the number of clusters or their shape.\n",
    "\n",
    "You don't have to select the k. \n",
    "\n",
    "---\n",
    "\n",
    "**Concept : **\n",
    "\n",
    "Imagine you are on a very foggy football field ( two dimensional feature space). with 100 people standing on it. (observations)\n",
    "\n",
    "Every minute each person take a step in the direction the more people they can see.\n",
    "\n",
    "As times goes on, people start to group up as they repeatedly take steps toward larger and larger crowds. \n",
    "\n",
    "The end result is clisters of people around the field and people are assigned to the cluster the end up.\n",
    "\n",
    "---\n",
    "\n",
    "band width : set the radius of the area an observation uses to determine the direction to shift.\n",
    "\n",
    "how far a person can see in the fog\n",
    "\n",
    "sometimes there are no other observations within an observation kernel. Meanshift assing all this orphan values to the closest observation's kernel\n",
    "\n",
    "cluster_all 0 False : orphan observations are given the label of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = MeanShift(n_jobs =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=None, bin_seeding=False, cluster_all=True, max_iter=300,\n",
       "          min_bin_freq=1, n_jobs=-1, seeds=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = cluster.fit(features_standardized)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using DBSCAN\n",
    "\n",
    "Group observations into clusters of high density.\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "Motivated by the idea that clusters will be areas where many observations are densely packer together and makes no assumption of cluster shape.\n",
    "\n",
    "- 1-. A random observation is chosen\n",
    " \n",
    "- 2-. If X has a minimum number of close neighbors we consider it to be part of a cluster\n",
    "\n",
    "- 3-. Previous step is repeated recursively for all xś neighbors, then neighbor's neighbor and so on. These are the cluster's core observations.\n",
    "\n",
    "- 4-. Once step 3 euns out of nearby observations, a new random point is chosen (restarting step 1)\n",
    "\n",
    "Any observation close to the cluster but not a core sample will be considered as part of the cluster.\n",
    "\n",
    "**Parameters**\n",
    "\n",
    "eps: Maximum distance fron an observation for another observation to be considered its neighbors.\n",
    "\n",
    "min_samples: The minimum number of observations less than eps distance from an observation fot it to be considered a core observation.\n",
    "\n",
    "metric : Distance metric used. minkowski (p intensity), euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DBSCAN object\n",
    "\n",
    "cluster = DBSCAN(n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.5, leaf_size=30, metric='euclidean',\n",
       "       metric_params=None, min_samples=5, n_jobs=-1, p=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = cluster.fit(features_standardized)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1, -1,\n",
       "        0,  0,  0,  0,  0,  0,  0, -1,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        1,  1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1,  1,  1,  1,  1,  1,\n",
       "       -1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "       -1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1, -1,  1,\n",
       "        1,  1,  1, -1, -1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1, -1,\n",
       "       -1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1, -1,\n",
       "       -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Using Hierarchical Merging\n",
    "\n",
    "Group observations using hierarchy of clusters.\n",
    "\n",
    "### Agglomerative Clustering.\n",
    "\n",
    "Powerful, flexible hierarchical clustering algorithm.\n",
    "\n",
    "All observations starts as their own clusters.\n",
    "\n",
    "Clusters meeting some criteria are merged together.\n",
    "\n",
    "This process is repeated, growing cluster until some end point is reached. Use linkage parameter to determine merging strategy to minimize the following:\n",
    "\n",
    "- Variance of merged clusters\n",
    "- Average distance between observations from pairs of clusters (average)\n",
    "- Maximun distance between observations from pairs of clusters (complete)\n",
    "\n",
    "Affinity : Determines the distance metric used for linkage. minkowski, euclidean\n",
    "n_clusters : number of cluster it will attempt to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DBSCAN object\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='euclidean', compute_full_tree='auto',\n",
       "                        connectivity=None, distance_threshold=None,\n",
       "                        linkage='ward', memory=None, n_clusters=3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "model = cluster.fit(features_standardized)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
       "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kikenv",
   "language": "python",
   "name": "kikenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
