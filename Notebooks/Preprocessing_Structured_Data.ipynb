{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Pandas Categorical Data For Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = {'patient':[1,2,3,4,5],\n",
    "           'obs' : [1,2,3,1,2],\n",
    "           'treatment' : [0,1,0,1,0],\n",
    "           'score': ['strong','weak', 'normal', 'weak', 'strong']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame(raw_data, columns = ['patient', 'obs', 'treatment', 'score'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the label encoder\n",
    "\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the encoder to the pandas column\n",
    "\n",
    "le.fit(df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View the labels \n",
    "\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform Categories Into Integers\n",
    "le.transform(df['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform Integers into their category names\n",
    "\n",
    "list(le.inverse_transform([2,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Observations With Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "X = np.array([[1.1, 11.1], \n",
    "              [2.2, 22.2], \n",
    "              [3.3, 33.3], \n",
    "              [4.4, 44.4],\n",
    "              [8.8, 88.8],\n",
    "              [np.nan, 55]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Observations With Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove observations with missing values\n",
    "X[~np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Data Frame\n",
    "X = np.array([[1.11111,2],\n",
    "             [6,3.3],\n",
    "             [8.8,4],\n",
    "             [9,5.5],\n",
    "             [np.nan,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop Missing Values Using NumPy\n",
    "X[~np.isnan(X).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop Missing Values using PANDAS\n",
    "\n",
    "df = pd.DataFrame(X, columns=['cosa_buena', 'cosa_mala'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove observations with missing values \n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np \n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Data\n",
    "\n",
    "X, _ = make_blobs(n_samples = 10,\n",
    "                 n_features = 2 ,\n",
    "                 centers = 1 ,\n",
    "                 random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the first observationś values with extreme values\n",
    "\n",
    "X[0,0] = 10000\n",
    "X[0,1] = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Outliers\n",
    "\n",
    "\n",
    "EllipticEnvelope assumes the data is normally distributed and based on that assumption “draws” an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1). A major limitation of this approach is the need to specify a contamination parameter which is the proportion of observations that are outliers, a value that we don’t know.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Detector \n",
    "\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "outlier_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit Detector \n",
    "outlier_detector.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict Outliers\n",
    "\n",
    "outlier_detector.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretize Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature\n",
    "age = np.array([[6], \n",
    "                [12], \n",
    "                [20], \n",
    "                [36], \n",
    "                [65]])\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Binarize Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create binarizer\n",
    "binarizer = Binarizer(18)\n",
    "\n",
    "binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform feature\n",
    "binarizer.fit_transform(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Option 2: Break Up Feature Into Bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bin feature\n",
    "np.digitize(age, bins=[20,30,64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Encoding Ordinal Categorical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convertir una variable categórica en integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading libraries \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "\n",
    "df = pd.DataFrame({'Score': ['Low', 'Low', 'Medium', 'Medium', 'High']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Scale Map ( Mapper) \n",
    "\n",
    "scale_mapper = {'Low':1,\n",
    "               'Medium':2,\n",
    "               'High': 3}\n",
    "scale_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map scale to Features\n",
    "# Map feature values to scale\n",
    "\n",
    "df['Scale'] = df['Score'].replace(scale_mapper)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Classes With Downsampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNSAMPLING : \n",
    "\n",
    "#### A strategy to handle imbalanced classes by creating a random subset of the majority of equal size to minority class.\n",
    "In downsampling, we randomly sample without replacement from the majority class (i.e. the class with more observations) to create a new subset of observation equal in size to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load df\n",
    "\n",
    "iris = load_iris()\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "\n",
    "X = iris.data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create target vector\n",
    "\n",
    "y = iris.target\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Iris Data Set Imbalanced \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove first 40 observations\n",
    "\n",
    "X = X[40:,:]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove first 40 observations \n",
    "\n",
    "y = y[40:]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a binary target vector indicating if class 0\n",
    "\n",
    "y = np.where((y == 0), 0, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample majority class to match minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indicies of each class' observations\n",
    "\n",
    "i_class0 = np.where(y == 0)[0]\n",
    "i_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Indicies of each class' observations\n",
    "\n",
    "i_class1 = np.where(y== 1)[0]\n",
    "i_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of observations in each class\n",
    "\n",
    "n_class0 = len(i_class0)\n",
    "n_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of observations in each class\n",
    "\n",
    "n_class1 = len(i_class1)\n",
    "n_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For every observation of class 0, randomly sample from class 1 without replacement\n",
    "\n",
    "i_class1_downsampled = np.random.choice(i_class1, size = n_class0, replace = False)\n",
    "i_class1_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join together class 0ś target vector with teh downsampled class 1ś target vector\n",
    "\n",
    "np.hstack((y[i_class0], y[i_class1_downsampled]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Classes With Upsampling\n",
    "\n",
    "## UPSAMPLING\n",
    "\n",
    "#### A strategy  to handle imbalanced classes by repeatedly sample with replacement from the minority class to make it of equal size as the majority class. \n",
    "\n",
    "In upsampling, for every observation in the majority class, we randomly select an observation from the minority class with replacement. The end result is the same number of observations from the minority and majority classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load iris dataset\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Iris DataSet imbalanced\n",
    "\n",
    "X = X[40:,:]\n",
    "y = y[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create binary target vector indicating if class 0\n",
    "\n",
    "y = np.where((y==0), 0,1)\n",
    "\n",
    "# Look at the imbalanced target vector\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling Minority Class To Match Majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indicies of each class'observations \n",
    "\n",
    "i_class0 = np.where(y==0)[0]\n",
    "i_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Indicies of each class'observations \n",
    "\n",
    "i_class1 = np.where(y==1)[0]\n",
    "i_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of observations in each class\n",
    "\n",
    "n_class0 = len(i_class0)\n",
    "n_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of observations in each class\n",
    "\n",
    "n_class1 = len(i_class1)\n",
    "n_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For every observation in class 1, randomly sample from class 0 with replacement\n",
    "\n",
    "i_class0_UPsampled = np.random.choice(i_class0, size = n_class1, replace = True)   #### difference with DOWNsampling\n",
    "i_class0_UPsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join together calss 0ś upsampled target vector with class 1ś target vector\n",
    "\n",
    "np.concatenate((y[i_class0_UPsampled],y[i_class1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Outliers\n",
    "\n",
    "### Drop:\n",
    "Not a great option. We lose lots of information. Find out if genuine extreme value or broken sensor.\n",
    "\n",
    "### Mark:\n",
    "Safest option. We can see if the outliers had an effect.\n",
    "\n",
    "### Rescale: \n",
    "Log values so outliers don gave as great an effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "\n",
    "houses = pd.DataFrame()\n",
    "houses['Price'] = [1000,2000,3000,4000,50000]\n",
    "houses['Bathrooms'] = [2,3,4,4.5,160]\n",
    "houses['Square_Feet']= [1500, 2500, 3500, 2500, 48000]\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DROP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop observations greater than some value\n",
    "\n",
    "houses[houses['Bathrooms']<8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARK: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load library\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature based on boolean condition\n",
    "\n",
    "houses['Outlier'] = np.where(houses['Square_Feet'] < 3000,0,1)   #  Menor que 3000 y sustituit por 0 y 1\n",
    "houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESCALE:   ( TO LOGARITHMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log feature \n",
    "\n",
    "houses['Log_Of_Square_Feet'] = [np.log(x) for x in houses['Square_Feet']]\n",
    "houses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute Missing Values With Means\n",
    "\n",
    "Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most ‘naive’ imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data\n",
    "\n",
    "# Create an empty dataset\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Create twpo variables called x0 & x1. Make the first value of X1 a missing value\n",
    "\n",
    "df['x0'] = [0.3051,0.4949,0.6974,0.3769,0.2231,0.341,0.4436,0.5897,0.6308,0.5]\n",
    "df['x1'] = [np.nan,0.2654,0.2615,0.5846,0.4615,0.8308,0.4962,0.3269,0.5346,0.6731]\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Imputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an imputer object that looks for 'Nan' values, then replaces them with the mean value \n",
    "# of the features by columns (axis=0)\n",
    "\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy = 'mean', axis = 0)\n",
    "mean_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the inputor on the df dataset\n",
    "\n",
    "mean_imputer = mean_imputer.fit(df)\n",
    "mean_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the imputer to the df dataset\n",
    "\n",
    "imputed_df = mean_imputer.transform(df.values)\n",
    "\n",
    "imputed_df\n",
    "\n",
    "## 0.49273333 es el mean sustituido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing Class Labels\n",
    "##  Fill Missing Values’ Class With Most Frequent Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries \n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create feature matrix with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature matriz with categorical feature\n",
    "\n",
    "X = np.array([[0, 1.1, 2.2],\n",
    "             [1, 3.3, 4.4],\n",
    "             [0, 5.5, 6.6],\n",
    "             [0, -7.7, -8.8],\n",
    "             [np.nan, 0.87, 9.9],\n",
    "             [np.nan, -0.67, -0.22]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing Values’ Class With Most Frequent Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Imputer object \n",
    "\n",
    "imputer = Imputer(strategy='most_frequent', axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill missing values class with most frequest class\n",
    "\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing Class Labels Using k-Nearest Neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix with categorical feature\n",
    "\n",
    "X = np.array([[0, 2.10, 1.45], \n",
    "              [1, 1.18, 1.33], \n",
    "              [0, 1.22, 1.27],\n",
    "              [1, -0.21, -1.19]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature Matrix With Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  nan,  0.87,  1.31],\n",
       "       [  nan, -0.67, -0.22]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create feature matrix with missing values in the categorical feature\n",
    " \n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    "                      [np.nan, -0.67, -0.22]])\n",
    "X_with_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train k-Nearest Neighbor Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train KNN Learner\n",
    "\n",
    "clf = KNeighborsClassifier(3, weights = 'distance')\n",
    "\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict missing values'class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict missing values class'\n",
    "\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "imputed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Join column of predicted class with their other features\n",
    "\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join two feature matrices\n",
    "\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normalizing Observations\n",
    "\n",
    "#### Rescaling the feature values of each observation so that they have a unit form. Two common norm values are L1 and L2. ( Usually L2 is predicted as default)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.5 ,   0.5 ],\n",
       "       [  1.1 ,   3.4 ],\n",
       "       [  1.5 ,  20.2 ],\n",
       "       [  1.63,  34.4 ],\n",
       "       [ 10.9 ,   3.3 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix\n",
    "\n",
    "X = np.array([[0.5, 0.5],\n",
    "              [1.1, 3.4],\n",
    "              [1.5, 20.2],\n",
    "              [1.63, 34.4],\n",
    "              [10.9, 3.3]])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Normalize observations \n",
    "\n",
    "Normalizer rescales the values on individual observations to have unit form ( the sum of their lengths is one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  0.70710678],\n",
       "       [ 0.30782029,  0.95144452],\n",
       "       [ 0.07405353,  0.99725427],\n",
       "       [ 0.04733062,  0.99887928],\n",
       "       [ 0.95709822,  0.28976368]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Normalizer\n",
    "\n",
    "normalizer = Normalizer(norm='l2')  ## L2\n",
    "\n",
    "# Transform feature matrix \n",
    "\n",
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One-Hot Encode Features With Multiple Labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create NumPy Array\n",
    "\n",
    "y = [('Granada', 'Málaga'),\n",
    "    ('Zanzibar', 'Madrid'),\n",
    "    ('Zanzibar', 'Bali'),\n",
    "    ('Zanzibar', 'Granada'),\n",
    "    ('Bali', 'Granada')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### One-hot Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create MultiBalbelBinarizer object\n",
    "\n",
    "one_hot = MultiLabelBinarizer()\n",
    "\n",
    "# One-hot encode data\n",
    "\n",
    "one_hot.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bali', 'Granada', 'Madrid', 'Málaga', 'Zanzibar'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View column Headers\n",
    "\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encode Nominal Categorical Features\n",
    "\n",
    "### One Hot encoding allows us to turn nominal categorical data into features with numerical values, while not mathematically imply ani ordinal relationship between the classes.\n",
    "\n",
    "#### Example : feature( 'Apple','Pear', 'Apple','Pear', 'Apple')\n",
    "          Apple(1,0,1,0,1)\n",
    "          Pear (0,1,0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Data With One Class Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Granada'],\n",
       "       ['Bali'],\n",
       "       ['Granada'],\n",
       "       ['Zanzibar'],\n",
       "       ['Granada']],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create NumPy array\n",
    "x = np.array([['Granada'],\n",
    "              ['Bali'],\n",
    "              ['Granada'],\n",
    "              ['Zanzibar'],\n",
    "              ['Granada']])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encode Data (Method 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LabelBinarizer object\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "\n",
    "# One hot encode data\n",
    "\n",
    "one_hot.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bali', 'Granada', 'Zanzibar'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Columns Headers\n",
    "# View classes\n",
    "\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encode Data (Method 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bali</th>\n",
       "      <th>Granada</th>\n",
       "      <th>Zanzibar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bali  Granada  Zanzibar\n",
       "0     0        1         0\n",
       "1     1        0         0\n",
       "2     0        1         0\n",
       "3     0        0         1\n",
       "4     0        1         0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy Feature\n",
    "\n",
    "pd.get_dummies(x[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Categorical Features\n",
    "\n",
    "### Often, machine learning methods (e.g. logistic regression, SVM with a linear kernel, etc) will require that categorical variables be converted into dummy variables (also called OneHot encoding). For example, a single feature Fruit would be converted into three features, Apples, Oranges, and Bananas, one for each category in the categorical feature.\n",
    "\n",
    "There are common ways to preprocess categorical features: using pandas or scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kike</td>\n",
       "      <td>Pedrinaci</td>\n",
       "      <td>29</td>\n",
       "      <td>Granada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nuno</td>\n",
       "      <td>Carvalho</td>\n",
       "      <td>25</td>\n",
       "      <td>Lisboa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carlos</td>\n",
       "      <td>Rosado</td>\n",
       "      <td>28</td>\n",
       "      <td>Ávila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Messi</td>\n",
       "      <td>D10S</td>\n",
       "      <td>31</td>\n",
       "      <td>Rosario</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coutinho</td>\n",
       "      <td>Correia</td>\n",
       "      <td>27</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_name  last_name age            city\n",
       "0       Kike  Pedrinaci  29         Granada\n",
       "1       Nuno   Carvalho  25          Lisboa\n",
       "2     Carlos     Rosado  28           Ávila\n",
       "3      Messi       D10S  31         Rosario\n",
       "4   Coutinho    Correia  27  Rio de Janeiro"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = {'first_name': ['Kike', 'Nuno', 'Carlos', 'Messi', 'Coutinho'],\n",
    "           'last_name': ['Pedrinaci','Carvalho', 'Rosado', 'D10S', 'Correia'],\n",
    "           'age': ['29', '25', '28', '31', '27'],\n",
    "           'city' : ['Granada', 'Lisboa', 'Ávila', 'Rosario','Rio de Janeiro']\n",
    "           }\n",
    "df = pd.DataFrame(raw_data, columns = ['first_name', 'last_name', 'age', 'city'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Nominal Categorical Feature Into Dummy Variables Using Pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Granada</th>\n",
       "      <th>Lisboa</th>\n",
       "      <th>Rio de Janeiro</th>\n",
       "      <th>Rosario</th>\n",
       "      <th>Ávila</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Granada  Lisboa  Rio de Janeiro  Rosario  Ávila\n",
       "0        1       0               0        0      0\n",
       "1        0       1               0        0      0\n",
       "2        0       0               0        0      1\n",
       "3        0       0               0        1      0\n",
       "4        0       0               1        0      0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dummy variales for every unique category in df.city\n",
    "\n",
    "pd.get_dummies(df['city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Nominal Categorical Data Into Dummy (OneHot) Features Using Scikit\n",
    "\n",
    "#### Note that the output of pd.get_dummies() and the scikit methods produces the same output matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 3, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert strings categorical names to integers\n",
    "\n",
    "integerized_data = preprocessing.LabelEncoder().fit_transform(df['city'])\n",
    "integerized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert integer categorical representations to OneHot encodings\n",
    "\n",
    "preprocessing.OneHotEncoder().fit_transform(integerized_data.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Iris Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kike/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data For Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Random split the data into four new datasets, training features, training outcome, test features, \n",
    "# and test outcome. Set the size of the test data to be 30% of the full dataset.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Feature Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load standard scaler \n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard desviation based on the training data\n",
    "\n",
    "sc.fit(X_train)\n",
    "\n",
    "# Scale the training data to be of mean 0 and of unit variance\n",
    "\n",
    "X_train_std = sc.transform(X_train)\n",
    "\n",
    "# Scale the test data to be of mean 0 and of unit variance\n",
    "\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 6.8,  2.8,  4.8,  1.4]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Test Data, non standarized\n",
    "\n",
    "X_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3100623 , -0.49582097,  0.48403749, -0.05143998],\n",
       "       [-0.17225683,  1.92563026, -1.26851205, -1.26670948],\n",
       "       [ 2.23933883, -0.98011121,  1.76924049,  1.43388941],\n",
       "       [ 0.18948252, -0.25367584,  0.36720086,  0.35364985],\n",
       "       [ 1.15412078, -0.49582097,  0.54245581,  0.21861991]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Test Data, standarized\n",
    "\n",
    "X_test_std[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescale A Feature\n",
    "### Min Max Scaling\n",
    "\n",
    "Rescales feature values to between (0,1). Rescaled value ==> X = (Original value - min(x) ) / (Max(x) - min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-500.5],\n",
       "       [-100.1],\n",
       "       [   0. ],\n",
       "       [ 100.1],\n",
       "       [ 900.9]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[-500.5],\n",
    "             [-100.1],\n",
    "             [0],\n",
    "             [100.1],\n",
    "             [900.9]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale Feature Using Min-Max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Scaler\n",
    "\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ],\n",
       "       [ 0.28571429],\n",
       "       [ 0.35714286],\n",
       "       [ 0.42857143],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale feature\n",
    "\n",
    "x_scale = minmax_scale.fit_transform(x)\n",
    "x_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize A Feature\n",
    "\n",
    "### Standardization is a common scaling method. Xi represents the number of standard deviations each value is from the mean value. It rescales a feature to hace a mean of 0 ad unit variance\n",
    "\n",
    "Xi = Xi - xº / Ó    [ standarized feature value = (value of the ith observation - mean of the feature vector )/ (standard deviation of feature vector)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-500.5],\n",
       "       [-100.1],\n",
       "       [   0. ],\n",
       "       [ 100.1],\n",
       "       [ 900.9]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature\n",
    "x = np.array([[-500.5], \n",
    "              [-100.1], \n",
    "              [0], \n",
    "              [100.1], \n",
    "              [900.9]])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create scaler\n",
    "\n",
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.26687088],\n",
       "       [-0.39316683],\n",
       "       [-0.17474081],\n",
       "       [ 0.0436852 ],\n",
       "       [ 1.79109332]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the feature\n",
    "\n",
    "standardized = scaler.fit_transform(x)\n",
    "standardized"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
